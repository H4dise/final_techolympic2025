{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=center style=\"line-height:200%;font-family:vazir;color:#0099cc\">\n",
    "<font face=\"vazir\" color=\"#0099cc\">\n",
    "TeaOverflowError\n",
    "</font>\n",
    "</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 dir=rtl align=right style=\"line-height:200%;font-family:vazir;color:#0099cc\">\n",
    "<font face=\"vazirmatn\" color=\"#0099cc\">\n",
    "مقدمه و صورت مسئله\n",
    "</font>\n",
    "</h2>\n",
    "\n",
    "\n",
    "<div dir=\"rtl\">\n",
    "<p dir=rtl style=\"direction: rtl; text-align: justify; line-height:200%; font-family:vazir; font-size:medium\">\n",
    "<font face=\"vazirmatn\" size=3>\n",
    "\n",
    "مدیرعامل شرکت معروف HonkHonk برای طراحی یک سیستم قیمت‌گذار پویا برای جایگزینی سیستم قبلی شرکت نیاز دارد. سیستم جدید باید تا حد امکان شبیه به سیستم قبلی باشد. به همین منظور مدیرعامل مجموعه داده‌ای برای یافتن نحوه قیمت‌گذاری سیستم قبلی در اختیار شما قرار داده است.\n",
    "\n",
    "با استفاده از این مجموعه داده و تکنیک‌های برنامه‌نویسی، هوش مصنوعی، یادگیری ماشین و جمع‌آوری داده‌های کمکی مورد نیاز بهترین سیستم ممکن برای جایگزینی این سیستم طراحی کنید.\n",
    "\n",
    "\n",
    "\n",
    "</font>\n",
    "</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 dir=rtl align=right style=\"line-height:200%;font-family:vazir;color:#0099cc\">\n",
    "<font face=\"vazir\" color=\"#0099cc\">\n",
    "معرفی مجموعه داده\n",
    "</font>\n",
    "</h2>\n",
    "\n",
    "<p dir=rtl style=\"direction: rtl; text-align: justify; line-height:200%; font-family:vazir; font-size:medium\">\n",
    "<font face=\"vazir\" size=3>\n",
    "در فایل اولیه‌ی این سوال، یک پوشه با نام <code>Data</code> وجود دارد که شامل ۲ فایل <code>train.csv</code> و <code>test.csv</code> است. این ۲ فایل شامل داده‌های از قیمت‌گذاری سیستم مرحوم برای سفرهای ثبت‌شده در سال ۲۰۱۶ در شهر نیویورک است.\n",
    "</font>\n",
    "</p>\n",
    "\n",
    "\n",
    "<p dir=rtl style=\"direction: rtl; text-align: justify; line-height:200%; font-family:vazir; font-size:medium\">\n",
    "<font face=\"vazir\" size=3>\n",
    "هرستون این مجموعه‌داده با توجه به ماهیت خود می‌تواند مقادیر عددی یا رشته‌ای داشته باشد. برای آشنایی بیشتر با مجموعه‌داده، پیشنهاد می‌شود هرستون به صورت جداگانه بررسی شود.</font>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4 dir=rtl align=right style=\"line-height:200%;font-family:vazir;color:#0099cc\">\n",
    "<font face=\"vazir\" color=\"#0099cc\">\n",
    "معرفی مجموعه داده آموزشی (train.csv)\n",
    "</font>\n",
    "</h4>\n",
    "\n",
    "<div dir=\"rtl\">\n",
    "<p dir=rtl style=\"direction: rtl; text-align: justify; line-height:200%; font-family:vazir; font-size:medium\">\n",
    "<font face=\"vazir\" size=3>\n",
    "\n",
    "فایل <code>train.csv</code> دارای  `11` ستون می‌باشد که ستون‌های آن به شرح زیر می‌باشد:\n",
    "\n",
    "<center>\n",
    "<div dir=rtl style=\"direction: rtl;line-height:200%;font-family:vazir;font-size:medium\">\n",
    "<font face=\"vazir\" size=3>\n",
    "    \n",
    "\n",
    "| نام ستون | توضیحات                                |\n",
    "| :----------------------: | :-----------------------------: |\n",
    "| `id`                    | یک شناسه منحصر به فرد برای هر سفر.                     |\n",
    "| `pickup_datetime`       | تاریخ و زمان شروع سفر.                               |\n",
    "| `dropoff_datetime`      | تاریخ و زمان پایان سفر.                               |\n",
    "| `passenger_count`       | تعداد مسافران در خودرو.                              |\n",
    "| `pickup_longitude`      | طول جغرافیایی مکان مبدا سفر.                         |\n",
    "| `pickup_latitude`       | عرض جغرافیایی مکان مبدا سفر.                         |\n",
    "| `dropoff_longitude`     | طول جغرافیایی مکان مقصد سفر.                         |\n",
    "| `dropoff_latitude`      | عرض جغرافیایی مکان مقصد سفر.                         |\n",
    "| `store_and_fwd_flag`    | نشان‌دهنده اینکه آیا اطلاعات سفر قبل از ارسال به سرور، در حافظه خودرو ذخیره شده بود یا خیر. (Y/N) |\n",
    "| `trip_duration`         | کل مدت زمان سفر به ثانیه.                           |\n",
    "| `total_price`           |  قیمت نهایی کل سفر به دلار  |\n",
    "\n",
    "\n",
    "</font>\n",
    "</div>\n",
    "</center>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4 dir=rtl align=right style=\"line-height:200%;font-family:vazir;color:#0099cc\">\n",
    "<font face=\"vazir\" color=\"#0099cc\">\n",
    "معرفی مجموعه داده آزمون (test.csv)\n",
    "</font>\n",
    "</h4>\n",
    "\n",
    "<div dir=\"rtl\">\n",
    "<p dir=rtl style=\"direction: rtl; text-align: justify; line-height:200%; font-family:vazir; font-size:medium\">\n",
    "<font face=\"vazir\" size=3>\n",
    "\n",
    "فایل <code>test.csv</code> ستون‌هایی مشابه با فایل `test.csv` دارد، با این تفاوت که فاقد ستون `total_price` می‌باشد.\n",
    "\n",
    "</font>\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p dir=rtl style=\"direction: rtl; text-align: justify; line-height:200%; font-family:vazir; font-size:medium\">\n",
    "<font face=\"vazir\" size=3>\n",
    "در این قسمت کتاب‌خانه‌ها و ابزار مورد نیاز خود را <code>import</code> کنید و فایل داده‌ها را که در پوشه‌ی <code>Data</code> و که با نام‌های <code>train.csv</code> و <code>test.csv</code> ذخیره‌شده‌اند را بخوانید و وارد محیط کار خود کنید.\n",
    "</font>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import lightgbm as lgb\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import joblib\n",
    "import warnings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>pickup_datetime</th>\n",
       "      <th>dropoff_datetime</th>\n",
       "      <th>passenger_count</th>\n",
       "      <th>pickup_longitude</th>\n",
       "      <th>pickup_latitude</th>\n",
       "      <th>dropoff_longitude</th>\n",
       "      <th>dropoff_latitude</th>\n",
       "      <th>store_and_fwd_flag</th>\n",
       "      <th>trip_duration</th>\n",
       "      <th>total_price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id3702621</td>\n",
       "      <td>2016-06-07 08:57:04</td>\n",
       "      <td>2016-06-07 09:05:47</td>\n",
       "      <td>1</td>\n",
       "      <td>-73.951935</td>\n",
       "      <td>40.777950</td>\n",
       "      <td>-73.960419</td>\n",
       "      <td>40.766243</td>\n",
       "      <td>N</td>\n",
       "      <td>523</td>\n",
       "      <td>14.045928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id2583306</td>\n",
       "      <td>2016-02-12 16:51:40</td>\n",
       "      <td>2016-02-12 16:56:23</td>\n",
       "      <td>2</td>\n",
       "      <td>-73.974289</td>\n",
       "      <td>40.742496</td>\n",
       "      <td>-73.971741</td>\n",
       "      <td>40.749649</td>\n",
       "      <td>N</td>\n",
       "      <td>283</td>\n",
       "      <td>15.291179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id0304728</td>\n",
       "      <td>2016-05-14 22:11:11</td>\n",
       "      <td>2016-05-14 22:14:49</td>\n",
       "      <td>1</td>\n",
       "      <td>-73.973183</td>\n",
       "      <td>40.748604</td>\n",
       "      <td>-73.979942</td>\n",
       "      <td>40.746174</td>\n",
       "      <td>N</td>\n",
       "      <td>218</td>\n",
       "      <td>12.236413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id0332187</td>\n",
       "      <td>2016-06-25 23:04:34</td>\n",
       "      <td>2016-06-25 23:12:28</td>\n",
       "      <td>3</td>\n",
       "      <td>-73.964890</td>\n",
       "      <td>40.761669</td>\n",
       "      <td>-73.989464</td>\n",
       "      <td>40.750519</td>\n",
       "      <td>N</td>\n",
       "      <td>474</td>\n",
       "      <td>20.959445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id0030703</td>\n",
       "      <td>2016-04-02 15:13:21</td>\n",
       "      <td>2016-04-02 15:19:17</td>\n",
       "      <td>1</td>\n",
       "      <td>-73.945518</td>\n",
       "      <td>40.778568</td>\n",
       "      <td>-73.956703</td>\n",
       "      <td>40.777046</td>\n",
       "      <td>N</td>\n",
       "      <td>356</td>\n",
       "      <td>10.866900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1255089</th>\n",
       "      <td>id2207278</td>\n",
       "      <td>2016-01-18 18:28:32</td>\n",
       "      <td>2016-01-18 18:41:54</td>\n",
       "      <td>1</td>\n",
       "      <td>-73.827515</td>\n",
       "      <td>40.683514</td>\n",
       "      <td>-73.827515</td>\n",
       "      <td>40.683514</td>\n",
       "      <td>N</td>\n",
       "      <td>802</td>\n",
       "      <td>15.672213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1255090</th>\n",
       "      <td>id1733189</td>\n",
       "      <td>2016-01-04 10:11:53</td>\n",
       "      <td>2016-01-04 10:19:45</td>\n",
       "      <td>5</td>\n",
       "      <td>-73.985191</td>\n",
       "      <td>40.723701</td>\n",
       "      <td>-73.985046</td>\n",
       "      <td>40.739201</td>\n",
       "      <td>N</td>\n",
       "      <td>472</td>\n",
       "      <td>27.773009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1255091</th>\n",
       "      <td>id0531568</td>\n",
       "      <td>2016-06-20 22:52:45</td>\n",
       "      <td>2016-06-20 22:56:27</td>\n",
       "      <td>2</td>\n",
       "      <td>-73.997543</td>\n",
       "      <td>40.756561</td>\n",
       "      <td>-73.988266</td>\n",
       "      <td>40.764267</td>\n",
       "      <td>N</td>\n",
       "      <td>222</td>\n",
       "      <td>9.682135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1255092</th>\n",
       "      <td>id3849280</td>\n",
       "      <td>2016-01-22 02:50:31</td>\n",
       "      <td>2016-01-22 03:08:24</td>\n",
       "      <td>1</td>\n",
       "      <td>-73.991287</td>\n",
       "      <td>40.755386</td>\n",
       "      <td>-73.948639</td>\n",
       "      <td>40.808270</td>\n",
       "      <td>N</td>\n",
       "      <td>1073</td>\n",
       "      <td>26.751263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1255093</th>\n",
       "      <td>id1909304</td>\n",
       "      <td>2016-05-03 18:16:00</td>\n",
       "      <td>2016-05-03 18:36:48</td>\n",
       "      <td>2</td>\n",
       "      <td>-73.980980</td>\n",
       "      <td>40.737656</td>\n",
       "      <td>-73.984016</td>\n",
       "      <td>40.757229</td>\n",
       "      <td>N</td>\n",
       "      <td>1248</td>\n",
       "      <td>27.261338</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1255094 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                id      pickup_datetime     dropoff_datetime  passenger_count  \\\n",
       "0        id3702621  2016-06-07 08:57:04  2016-06-07 09:05:47                1   \n",
       "1        id2583306  2016-02-12 16:51:40  2016-02-12 16:56:23                2   \n",
       "2        id0304728  2016-05-14 22:11:11  2016-05-14 22:14:49                1   \n",
       "3        id0332187  2016-06-25 23:04:34  2016-06-25 23:12:28                3   \n",
       "4        id0030703  2016-04-02 15:13:21  2016-04-02 15:19:17                1   \n",
       "...            ...                  ...                  ...              ...   \n",
       "1255089  id2207278  2016-01-18 18:28:32  2016-01-18 18:41:54                1   \n",
       "1255090  id1733189  2016-01-04 10:11:53  2016-01-04 10:19:45                5   \n",
       "1255091  id0531568  2016-06-20 22:52:45  2016-06-20 22:56:27                2   \n",
       "1255092  id3849280  2016-01-22 02:50:31  2016-01-22 03:08:24                1   \n",
       "1255093  id1909304  2016-05-03 18:16:00  2016-05-03 18:36:48                2   \n",
       "\n",
       "         pickup_longitude  pickup_latitude  dropoff_longitude  \\\n",
       "0              -73.951935        40.777950         -73.960419   \n",
       "1              -73.974289        40.742496         -73.971741   \n",
       "2              -73.973183        40.748604         -73.979942   \n",
       "3              -73.964890        40.761669         -73.989464   \n",
       "4              -73.945518        40.778568         -73.956703   \n",
       "...                   ...              ...                ...   \n",
       "1255089        -73.827515        40.683514         -73.827515   \n",
       "1255090        -73.985191        40.723701         -73.985046   \n",
       "1255091        -73.997543        40.756561         -73.988266   \n",
       "1255092        -73.991287        40.755386         -73.948639   \n",
       "1255093        -73.980980        40.737656         -73.984016   \n",
       "\n",
       "         dropoff_latitude store_and_fwd_flag  trip_duration  total_price  \n",
       "0               40.766243                  N            523    14.045928  \n",
       "1               40.749649                  N            283    15.291179  \n",
       "2               40.746174                  N            218    12.236413  \n",
       "3               40.750519                  N            474    20.959445  \n",
       "4               40.777046                  N            356    10.866900  \n",
       "...                   ...                ...            ...          ...  \n",
       "1255089         40.683514                  N            802    15.672213  \n",
       "1255090         40.739201                  N            472    27.773009  \n",
       "1255091         40.764267                  N            222     9.682135  \n",
       "1255092         40.808270                  N           1073    26.751263  \n",
       "1255093         40.757229                  N           1248    27.261338  \n",
       "\n",
       "[1255094 rows x 11 columns]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv(\"../Data/train.csv\")\n",
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "try:\n",
    "    train_df = pd.read_csv('../data/train.csv', parse_dates=['pickup_datetime','dropoff_datetime'])\n",
    "    test_df = pd.read_csv('../data/test.csv', parse_dates=['pickup_datetime','dropoff_datetime'])\n",
    "    print(\"Data loaded successfully.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: train.csv or test.csv not found. Make sure they are in the correct directory.\")\n",
    "    # Create empty placeholders if files are missing, to allow notebook to run\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 dir=rtl align=right style=\"line-height:200%;font-family:vazir;color:#0099cc\">\n",
    "<font face=\"vazir\" color=\"#0099cc\">\n",
    "پیش‌پردازش و مهندسی ویژگی\n",
    "</font>\n",
    "</h2>\n",
    "\n",
    "<p dir=rtl style=\"direction: rtl; text-align: justify; line-height:200%; font-family:vazir; font-size:medium\">\n",
    "<font face=\"vazir\" size=3>\n",
    "        در این سوال شما می‌توانید از هر تکنیک پیش‌پردازش/مهندسی ویژگی دلخواهتان، استفاده کنید.\n",
    "    <br>\n",
    "    تکنیک‌هایی که استفاده می‌کنید به شکل مستقیم مورد ارزیابی توسط سامانه داوری قرار <b>نمی‌گیرند.</b> بلکه همه آن‌ها در دقت مدل شما تاثیر خواهند گذاشت؛ بنابراین هر چه پیش‌پردازش/مهندسی ویژگی بهتری انجام دهید تا دقت مدل بهبود پیدا کند، امتیاز بیشتری از این سوال کسب خواهید کرد.\n",
    "    در این قسمت شما می‌توانید بخشی از داده‌ی موجود را برای اعتبارسنجی در نظر بگیرید.\n",
    "</font>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- 1. Helper Functions ----------\n",
    "\n",
    "def haversine_np(lon1, lat1, lon2, lat2):\n",
    "    R = 6371  # km\n",
    "    phi1 = np.radians(lat1)\n",
    "    phi2 = np.radians(lat2)\n",
    "    dphi = np.radians(lat2 - lat1)\n",
    "    dlambda = np.radians(lon2 - lon1)\n",
    "    a = np.sin(dphi/2.0)**2 + np.cos(phi1)*np.cos(phi2)*np.sin(dlambda/2.0)**2\n",
    "    return 2*R*np.arcsin(np.sqrt(a))\n",
    "\n",
    "def manhattan_np(lon1, lat1, lon2, lat2):\n",
    "    # Approximates Manhattan distance using lat/lon\n",
    "    # Uses degrees directly, scales by average km per degree\n",
    "    R_lat = 111.0  # km per degree latitude\n",
    "    R_lon = 85.0   # approx km per degree longitude at NYC lat\n",
    "    return R_lat * np.abs(lat2 - lat1) + R_lon * np.abs(lon2 - lon1)\n",
    "\n",
    "def rmse(y_true, y_pred):\n",
    "    return np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "\n",
    "def scoring_function(rmse_val, y_true):\n",
    "    std = np.std(y_true)\n",
    "    if std == 0: return 0.0\n",
    "    return 100.0 * np.exp(-rmse_val / std)\n",
    "\n",
    "# ---------- 2. Data Cleaning Function (Train only) ----------\n",
    "def clean_train_data(df):\n",
    "    # Keep original size for comparison\n",
    "    n_orig = len(df)\n",
    "\n",
    "    # Recalculate trip_duration for consistency\n",
    "    df['trip_duration'] = (df['dropoff_datetime'] - df['pickup_datetime']).dt.total_seconds()\n",
    "\n",
    "    # Apply filters\n",
    "    df = df[df['total_price'] > 1.0] # Remove $0 or negative fares\n",
    "    df = df[df['total_price'] < 500.0] # Remove extreme outliers\n",
    "    df = df[(df['trip_duration'] > 30) & (df['trip_duration'] < 3 * 3600)] # 30s to 3hr\n",
    "\n",
    "    # NYC Bounding Box\n",
    "    nyc_bb = {'min_lat': 40.5, 'max_lat': 41.0, 'min_lon': -74.3, 'max_lon': -73.7}\n",
    "    df = df[(df['pickup_latitude'] >= nyc_bb['min_lat']) & (df['pickup_latitude'] <= nyc_bb['max_lat'])]\n",
    "    df = df[(df['pickup_longitude'] >= nyc_bb['min_lon']) & (df['pickup_longitude'] <= nyc_bb['max_lon'])]\n",
    "    df = df[(df['dropoff_latitude'] >= nyc_bb['min_lat']) & (df['dropoff_latitude'] <= nyc_bb['max_lat'])]\n",
    "    df = df[(df['dropoff_longitude'] >= nyc_bb['min_lon']) & (df['dropoff_longitude'] <= nyc_bb['max_lon'])]\n",
    "\n",
    "    print(f\"Cleaned training data: {n_orig} -> {len(df)} rows\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned training data: 1255094 -> 1248246 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Lib\\site-packages\\sklearn\\utils\\validation.py:2742: UserWarning: X has feature names, but MiniBatchKMeans was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Lib\\site-packages\\sklearn\\utils\\validation.py:2742: UserWarning: X has feature names, but MiniBatchKMeans was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating aggregation features from training data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Lib\\site-packages\\sklearn\\utils\\validation.py:2742: UserWarning: X has feature names, but MiniBatchKMeans was fitted without feature names\n",
      "  warnings.warn(\n",
      "D:\\Lib\\site-packages\\sklearn\\utils\\validation.py:2742: UserWarning: X has feature names, but MiniBatchKMeans was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 24 features.\n",
      "Preprocessing and Feature Engineering complete.\n"
     ]
    }
   ],
   "source": [
    "# ---------- 3. Feature Engineering Function (Train & Test) ----------\n",
    "def create_features(df, kmeans_model, train_aggs=None):\n",
    "    # Datetime features\n",
    "    df['pickup_dt'] = pd.to_datetime(df['pickup_datetime'])\n",
    "    if 'dropoff_datetime' in df.columns:\n",
    "         df['dropoff_dt'] = pd.to_datetime(df['dropoff_datetime'])\n",
    "         # Ensure consistent trip_duration\n",
    "         df['trip_duration'] = (df['dropoff_dt'] - df['pickup_dt']).dt.total_seconds().fillna(0)\n",
    "\n",
    "    df['pickup_hour'] = df['pickup_dt'].dt.hour\n",
    "    df['pickup_dow'] = df['pickup_dt'].dt.weekday\n",
    "    df['pickup_month'] = df['pickup_dt'].dt.month\n",
    "    df['pickup_minute'] = df['pickup_dt'].dt.minute\n",
    "    df['is_weekend'] = (df['pickup_dow'] >= 5).astype(int)\n",
    "    # Rush hour (e.g., 7-9am, 4-6pm on weekdays)\n",
    "    df['is_rush_hour'] = (((df['pickup_hour'] >= 7) & (df['pickup_hour'] <= 9)) |\n",
    "                         ((df['pickup_hour'] >= 16) & (df['pickup_hour'] <= 18))) & \\\n",
    "                        (df['is_weekend'] == 0)\n",
    "    df['is_rush_hour'] = df['is_rush_hour'].astype(int)\n",
    "\n",
    "    # Distance features\n",
    "    df['dist_haversine'] = haversine_np(df['pickup_longitude'], df['pickup_latitude'],\n",
    "                                        df['dropoff_longitude'], df['dropoff_latitude'])\n",
    "    df['dist_manhattan'] = manhattan_np(df['pickup_longitude'], df['pickup_latitude'],\n",
    "                                        df['dropoff_longitude'], df['dropoff_latitude'])\n",
    "\n",
    "    # Filter 0-distance trips after calculation (only applied to the original train set, not the returned df)\n",
    "    # This filter line is removed from here as it should be done in cleaning_train_data\n",
    "    # For now, we'll keep the distance > 0.01 check only in the cleaning function, or rely on a robust model.\n",
    "\n",
    "    # Speed\n",
    "    # NOTE: trip_duration in test set is 0/missing. We only calculate this for train_df for aggregation\n",
    "    if 'trip_duration' in df.columns and len(df[df['trip_duration'] > 0]) > 0:\n",
    "        df['speed_kmh'] = df['dist_haversine'] / (df['trip_duration'] / 3600 + 1e-6)\n",
    "        # Clip absurd speeds\n",
    "        df['speed_kmh'] = df['speed_kmh'].clip(0, 100)\n",
    "    else:\n",
    "        # For test set where trip_duration is not present/zero, set speed to a placeholder\n",
    "        df['speed_kmh'] = 0.0\n",
    "\n",
    "    # Log transforms\n",
    "    if 'trip_duration' in df.columns:\n",
    "        df['log_trip_duration'] = np.log1p(df['trip_duration'])\n",
    "    df['log_dist_haversine'] = np.log1p(df['dist_haversine'])\n",
    "    df['log_dist_manhattan'] = np.log1p(df['dist_manhattan'])\n",
    "\n",
    "    # Flags\n",
    "    df['store_and_fwd_flag'] = df['store_and_fwd_flag'].fillna('N')\n",
    "    df['saf_flag'] = (df['store_and_fwd_flag'] == 'Y').astype(int)\n",
    "    df['passenger_count'] = df['passenger_count'].fillna(1).clip(1, 6)\n",
    "\n",
    "    # Spatial Clusters (using the fitted kmeans)\n",
    "    # The columns must be in (latitude, longitude) order if MiniBatchKMeans was fitted that way.\n",
    "    # The original fit was on vstack([[...latitude, ...longitude], [...latitude, ...longitude]]), so lat, lon is correct.\n",
    "    df['pickup_cluster'] = kmeans_model.predict(df[['pickup_latitude','pickup_longitude']])\n",
    "    df['dropoff_cluster'] = kmeans_model.predict(df[['dropoff_latitude','dropoff_longitude']])\n",
    "    # ** NEW: Cluster Interaction **\n",
    "    df['route_cluster'] = df['pickup_cluster'].astype(str) + '_' + df['dropoff_cluster'].astype(str)\n",
    "\n",
    "    # --- Aggregation Features ---\n",
    "    if train_aggs is None:\n",
    "        # This is the training run, create the aggregations\n",
    "        print(\"Calculating aggregation features from training data...\")\n",
    "        train_aggs = {}\n",
    "\n",
    "        # 1. Agg by time (for traffic/speed)\n",
    "        # Only aggregate if 'speed_kmh' exists (i.e., this is train_df)\n",
    "        if 'speed_kmh' in df.columns:\n",
    "            agg_time = df.groupby(['pickup_dow', 'pickup_hour'])['speed_kmh'].agg(['mean', 'std']).reset_index()\n",
    "            agg_time.columns = ['pickup_dow', 'pickup_hour', 'time_speed_mean', 'time_speed_std']\n",
    "            train_aggs['time'] = agg_time\n",
    "\n",
    "        # 2. Agg by pickup cluster (for demand)\n",
    "        agg_p_cluster = df.groupby('pickup_cluster')['total_price'].agg(['mean', 'std']).reset_index()\n",
    "        agg_p_cluster.columns = ['pickup_cluster', 'p_cluster_price_mean', 'p_cluster_price_std']\n",
    "        train_aggs['p_cluster'] = agg_p_cluster\n",
    "\n",
    "        # 3. Agg by route (for route-specific price/duration)\n",
    "        if 'trip_duration' in df.columns:\n",
    "            agg_route = df.groupby('route_cluster')['trip_duration'].agg(['mean', 'count']).reset_index()\n",
    "            agg_route.columns = ['route_cluster', 'route_duration_mean', 'route_count']\n",
    "            train_aggs['route'] = agg_route\n",
    "\n",
    "        train_aggs['global_speed_mean'] = df['speed_kmh'].mean() if 'speed_kmh' in df.columns else 0.0\n",
    "        train_aggs['global_price_mean'] = df['total_price'].mean()\n",
    "        train_aggs['global_duration_mean'] = df['trip_duration'].mean() if 'trip_duration' in df.columns else 0.0\n",
    "\n",
    "    # Merge aggregations\n",
    "    df = df.merge(train_aggs['time'], on=['pickup_dow', 'pickup_hour'], how='left')\n",
    "    df = df.merge(train_aggs['p_cluster'], on='pickup_cluster', how='left')\n",
    "    df = df.merge(train_aggs['route'], on='route_cluster', how='left')\n",
    "\n",
    "    # Fill NaNs created by merging (e.g., a new route in test set)\n",
    "    df['time_speed_mean'] = df['time_speed_mean'].fillna(train_aggs['global_speed_mean'])\n",
    "    df['time_speed_std'] = df['time_speed_std'].fillna(0)\n",
    "    df['p_cluster_price_mean'] = df['p_cluster_price_mean'].fillna(train_aggs['global_price_mean'])\n",
    "    df['p_cluster_price_std'] = df['p_cluster_price_std'].fillna(0)\n",
    "    df['route_duration_mean'] = df['route_duration_mean'].fillna(train_aggs['global_duration_mean'])\n",
    "    df['route_count'] = df['route_count'].fillna(0)\n",
    "\n",
    "    return df, train_aggs\n",
    "\n",
    "# ---------- 4. Execution Pipeline ----------\n",
    "# NOTE: Assume Cell 24 (train_df = pd.DataFrame()) has been deleted or commented out.\n",
    "# NOTE: The clean_train_data definition in Cell 26 is simplified and relies on the full definition in Cell 25.\n",
    "# Re-define cleaning function to use the full one from Cell 25\n",
    "def clean_train_data(df):\n",
    "    # Keep original size for comparison\n",
    "    n_orig = len(df)\n",
    "    # Recalculate trip_duration for consistency\n",
    "    df['trip_duration'] = (df['dropoff_datetime'] - df['pickup_datetime']).dt.total_seconds()\n",
    "    # Apply filters\n",
    "    df = df[df['total_price'] > 1.0] # Remove $0 or negative fares\n",
    "    df = df[df['total_price'] < 500.0] # Remove extreme outliers\n",
    "    df = df[(df['trip_duration'] > 30) & (df['trip_duration'] < 3 * 3600)] # 30s to 3hr\n",
    "    # NYC Bounding Box\n",
    "    nyc_bb = {'min_lat': 40.5, 'max_lat': 41.0, 'min_lon': -74.3, 'max_lon': -73.7}\n",
    "    df = df[(df['pickup_latitude'] >= nyc_bb['min_lat']) & (df['pickup_latitude'] <= nyc_bb['max_lat'])]\n",
    "    df = df[(df['pickup_longitude'] >= nyc_bb['min_lon']) & (df['pickup_longitude'] <= nyc_bb['max_lon'])]\n",
    "    df = df[(df['dropoff_latitude'] >= nyc_bb['min_lat']) & (df['dropoff_latitude'] <= nyc_bb['max_lat'])]\n",
    "    df = df[(df['dropoff_longitude'] >= nyc_bb['min_lon']) & (df['dropoff_longitude'] <= nyc_bb['max_lon'])]\n",
    "    print(f\"Cleaned training data: {n_orig} -> {len(df)} rows\")\n",
    "    return df\n",
    "\n",
    "# Clean train data\n",
    "train_df = clean_train_data(train_df)\n",
    "\n",
    "\n",
    "# Concat train and test for consistent clustering\n",
    "df_all = pd.concat([train_df, test_df], sort=False)\n",
    "\n",
    "# CORRECTED: Select Latitude AND Longitude for K-Means fitting\n",
    "coords = np.vstack([df_all[['pickup_latitude', 'pickup_longitude']].values,\n",
    "                    df_all[['dropoff_latitude', 'dropoff_longitude']].values])\n",
    "\n",
    "# Fit K-Means\n",
    "k = 100 # Increased clusters\n",
    "kmeans = MiniBatchKMeans(n_clusters=k, random_state=42, batch_size=10000).fit(coords)\n",
    "\n",
    "# Apply features to train, this will create the 'train_aggs' dictionary\n",
    "train_fe, train_aggs = create_features(train_df, kmeans)\n",
    "\n",
    "# Apply features to test, using the aggregations learned from train\n",
    "test_fe, _ = create_features(test_df, kmeans, train_aggs=train_aggs)\n",
    "\n",
    "# CORRECTED: Label Encode the 'route_cluster' feature by concatenating the featurized columns\n",
    "all_route_clusters = pd.concat([train_fe['route_cluster'], test_fe['route_cluster']], sort=False)\n",
    "route_encoder = LabelEncoder().fit(all_route_clusters)\n",
    "\n",
    "# Transform the features\n",
    "train_fe['route_cluster'] = route_encoder.transform(train_fe['route_cluster'])\n",
    "test_fe['route_cluster'] = route_encoder.transform(test_fe['route_cluster'])\n",
    "\n",
    "# ---------- 5. Define Feature Set and Target ----------\n",
    "features = [\n",
    "    'passenger_count', 'dist_haversine', 'log_dist_haversine',\n",
    "    'dist_manhattan', 'log_dist_manhattan',\n",
    "    'trip_duration', 'log_trip_duration', 'speed_kmh',\n",
    "    'pickup_hour', 'pickup_dow', 'pickup_month', 'pickup_minute',\n",
    "    'is_weekend', 'is_rush_hour', 'saf_flag',\n",
    "    'pickup_cluster', 'dropoff_cluster', 'route_cluster',\n",
    "    'time_speed_mean', 'time_speed_std',\n",
    "    'p_cluster_price_mean', 'p_cluster_price_std',\n",
    "    'route_duration_mean', 'route_count'\n",
    "]\n",
    "\n",
    "# Ensure no features were lost\n",
    "features = [f for f in features if f in train_fe.columns and f in test_fe.columns]\n",
    "print(f\"Using {len(features)} features.\")\n",
    "\n",
    "# Sort train data by time for TimeSeriesSplit\n",
    "train_sorted = train_fe.sort_values('pickup_dt').reset_index(drop=True)\n",
    "\n",
    "X = train_sorted[features]\n",
    "y = train_sorted['total_price']\n",
    "y_t = np.log1p(y) # Use log-transformed target\n",
    "\n",
    "X_test = test_fe[features]\n",
    "\n",
    "print(\"Preprocessing and Feature Engineering complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 dir=rtl align=right style=\"line-height:200%;font-family:vazir;color:#0099cc\">\n",
    "<font face=\"vazir\" color=\"#0099cc\">\n",
    "آموزش مدل\n",
    "</font>\n",
    "</h2>\n",
    "\n",
    "<p dir=rtl style=\"direction: rtl;text-align: justify;line-height:200%;font-family:vazir;font-size:medium\">\n",
    "<font face=\"vazir\" size=3>\n",
    "    حال که داده را پاکسازی کرده و احتمالا ویژگی‌هایی را به آن افزوده یا از آن حذف کرده‌اید، وقت آن است که مدلی آموزش دهید که بتواند متغیر هدف این مسئله را پیش‌بینی کند. در این قسمت استفاده از هر نوع مدل مربوط به یادگیری ماشین کلاسیک مجاز می‌باشد.\n",
    "    در این قسمت مدل مورد نظر باید طوری آموزش داده شود که قادر به پیش‌بینی مقادیر ستون <code>rent</code> باشد.\n",
    "</font>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting final LightGBM model training on all data...\n",
      "Model training complete.\n",
      "\n",
      "Submission DataFrame Head (Final):\n",
      "   total_price\n",
      "0    13.422268\n",
      "1    16.486478\n",
      "2    13.175807\n",
      "3    38.375695\n",
      "4    23.391207\n",
      "\n",
      "Submission file 'submission.csv' created.\n"
     ]
    }
   ],
   "source": [
    "# --- Replacement Code for Model Training (Single Model Approach) ---\n",
    "\n",
    "# LightGBM model parameters (Tuned for performance)\n",
    "params = {\n",
    "    'boosting_type': 'gbdt',\n",
    "    'objective': 'regression',\n",
    "    'metric': 'rmse',\n",
    "    'num_leaves': 31,\n",
    "    'learning_rate': 0.05,\n",
    "    'feature_fraction': 0.9,\n",
    "    'bagging_fraction': 0.8,\n",
    "    'bagging_freq': 5,\n",
    "    'verbose': -1,\n",
    "    'nthread': -1,\n",
    "    'seed': 42\n",
    "}\n",
    "\n",
    "# 1. Prepare Data for Final Model\n",
    "# X: all training features\n",
    "# y_t: all log-transformed training target (total_price)\n",
    "# X_test: test features\n",
    "\n",
    "lgb_train = lgb.Dataset(X, y_t)\n",
    "\n",
    "print(\"Starting final LightGBM model training on all data...\")\n",
    "\n",
    "# Train the final model with full data for the best generalization\n",
    "# Use a reasonable number of boosting rounds\n",
    "final_gbm = lgb.train(params,\n",
    "                      lgb_train,\n",
    "                      num_boost_round=1000)\n",
    "\n",
    "print(\"Model training complete.\")\n",
    "\n",
    "# 2. Final Prediction\n",
    "# Generate predictions on the entire test set\n",
    "test_preds = final_gbm.predict(X_test, num_iteration=final_gbm.best_iteration)\n",
    "\n",
    "# --- Final Evaluation (Simplified to use a train/val split if needed, \n",
    "# --- but we skip OOF calculation for simplicity) ---\n",
    "\n",
    "# Since we trained a single model on all data, we don't have 'oof_preds'\n",
    "# We will skip the OOF evaluation section entirely, which will eliminate the 'else section appeared why' issue.\n",
    "\n",
    "# --- Submission (Cell 38 equivalent) ---\n",
    "# The test predictions are log-transformed, so we inverse transform them\n",
    "final_predictions = np.expm1(test_preds)\n",
    "\n",
    "# Ensure no negative prices\n",
    "final_predictions[final_predictions < 0] = 0\n",
    "\n",
    "submission = pd.DataFrame({\n",
    "    'total_price': final_predictions\n",
    "})\n",
    "\n",
    "# Display submission head\n",
    "print(\"\\nSubmission DataFrame Head (Final):\")\n",
    "print(submission.head())\n",
    "\n",
    "# Save and Compress for Submission\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "print(\"\\nSubmission file 'submission.csv' created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting temporary R2 model training (using RFR)...\n",
      "Temporary R2 model training complete.\n",
      "\n",
      "--- PROXY VALIDATION SCORE ---\n",
      "R2 Score on 20% Validation Data (original $): 0.9527\n",
      "RMSE Score on 20% Validation Data (original $): 3.4372\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.ensemble import RandomForestRegressor # Substitute for LightGBM\n",
    "\n",
    "# --- Temporary Validation Split and R2 Calculation ---\n",
    "\n",
    "# 1. Split the data (using 80% for training and 20% for validation)\n",
    "X_train_val, X_val, y_train_val, y_val = train_test_split(\n",
    "    X, y_t, test_size=0.2, shuffle=False # Use shuffle=False for time-series data\n",
    ")\n",
    "\n",
    "# 2. Train a temporary model (using the RFR substitute with reduced settings for speed)\n",
    "temp_model = RandomForestRegressor(n_estimators=50, max_depth=8, random_state=42, n_jobs=-1)\n",
    "print(\"Starting temporary R2 model training (using RFR)...\")\n",
    "temp_model.fit(X_train_val, y_train_val)\n",
    "print(\"Temporary R2 model training complete.\")\n",
    "\n",
    "# 3. Predict on the validation set (log-transformed)\n",
    "val_preds_log = temp_model.predict(X_val)\n",
    "\n",
    "# 4. Calculate R2 score (in original $ space)\n",
    "y_val_orig = np.expm1(y_val)\n",
    "val_preds_orig = np.expm1(val_preds_log)\n",
    "val_preds_orig[val_preds_orig < 0] = 0\n",
    "\n",
    "r2_score_val = r2_score(y_val_orig, val_preds_orig)\n",
    "rmse_score_val = np.sqrt(mean_squared_error(y_val_orig, val_preds_orig))\n",
    "\n",
    "print(\"\\n--- PROXY VALIDATION SCORE ---\")\n",
    "print(f\"R2 Score on 20% Validation Data (original $): {r2_score_val:.4f}\")\n",
    "print(f\"RMSE Score on 20% Validation Data (original $): {rmse_score_val:.4f}\")\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# The variables X, y_t, and X_test remain defined for the final LightGBM model below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 dir=rtl align=right style=\"line-height:200%;font-family:vazir;color:#0099cc\">\n",
    "<font face=\"vazir\" color=\"#0099cc\">\n",
    "معیار ارزیابی\n",
    "</font>\n",
    "</h2>\n",
    "\n",
    "<p dir=rtl style=\"direction: rtl; text-align: justify; line-height:200%; font-family:vazir; font-size:medium\">\n",
    "<font face=\"vazir\" size=3>\n",
    "    معیاری که برای ارزیابی عملکرد مدل انتخاب کرده‌ایم، <code>r2_score</code> نام دارد.\n",
    "    <br>\n",
    "    این معیار، سنجه ارزیابی کیفیت مدل شماست. به عبارت بهتر در سامانه داوری هم از همین معیار برای نمره‌دهی استفاده شده است.\n",
    "    <br>\n",
    "    پیشنهاد می‌شود با توجه به این معیار، عملکرد مدل خود را بر روی مجموعه‌ی آموزش یا اعتبارسنجی ارزیابی کنید.\n",
    "</font>\n",
    "</p>\n",
    "\n",
    "<p dir=rtl style=\"direction: rtl; text-align: justify; line-height:200%; font-family:vazir; font-size:medium\">\n",
    "<font color=\"red\"><b color='red'>توجه:</b></font>\n",
    "<font face=\"vazir\" size=3>\n",
    "    برای دریافت نمره از این سوال لازم است تا دقت مدل شما از آستانه‌ی ۰.۵ بیشتر باشد.\n",
    "    در صورتی که دقت مدل شما از ۰.۵ کمتر باشد نمره شما \n",
    "    <b>صفر</b>\n",
    "    خواهد شد و در غیر این صورت با فرمول زیر محاسبه می‌شود:\n",
    "</font>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# TODO:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 dir=rtl align=right style=\"line-height:200%;font-family:vazir;color:#0099cc\">\n",
    "<font face=\"vazir\" color=\"#0099cc\">\n",
    " پیش‌بینی برای داده تست و خروجی\n",
    "</font>\n",
    "</h2>\n",
    "\n",
    "<p dir=rtl style=\"direction: rtl;text-align: right;line-height:200%;font-family:vazir;font-size:medium\">\n",
    "<font face=\"vazir\" size=3>\n",
    "    پیش‌بینی مدل خود بر روی داده‌های آزمایش (<code>test.csv</code>) را در قالب یک <code>dataframe</code> در متغیری با نام <code>submission</code>ذخیره کنید.  این <code>dataframe</code> باید دارای یک ستون با نام <code>total_price</code> باشد که ردیف <code>i</code>ام آن، پیش‌بینی شما برای سطر <code>i</code>ام مجموعه‌داده‌ی آزمون باشد.\n",
    "</font>\n",
    "</p>\n",
    "\n",
    "<center>\n",
    "<div dir=rtl style=\"direction: rtl;line-height:200%;font-family:vazir;font-size:medium\">\n",
    "<font face=\"vazir\" size=3>\n",
    "    \n",
    "|ستون|توضیحات|\n",
    "|------|---|\n",
    "|total_price|قیمت نهایی کل سفر به دلار|\n",
    "    \n",
    "</font>\n",
    "</div>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO:\n",
    "submission = pd.DataFrame({\n",
    "    'total_price': final_predictions\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 dir=rtl align=right style=\"line-height:200%;font-family:vazir;color:#0099cc\">\n",
    "<font face=\"vazir\" color=\"#0099cc\">\n",
    "<b>سلول جواب‌ساز</b>\n",
    "</font>\n",
    "</h2>\n",
    "\n",
    "<p dir=rtl style=\"direction: rtl; text-align: justify; line-height:200%; font-family:vazir; font-size:medium\">\n",
    "<font face=\"vazir\" size=3>\n",
    "    برای ساخته‌شدن فایل <code>result.zip</code> سلول زیر را اجرا کنید. توجه داشته باشید که پیش از اجرای سلول زیر تغییرات اعمال شده در نت‌بوک را ذخیره کرده باشید (<code>ctrl+s</code>) در غیر این صورت، در پایان مسابقه نمره شما به صفر تغییر خواهد کرد.\n",
    "    <br>\n",
    "    همچنین اگر از کولب برای اجرای این فایل نوت‌بوک استفاده می‌کنید، قبل از ارسال فایل <code>result.zip</code>، آخرین نسخه‌ی نوت‌بوک خود را دانلود کرده و داخل فایل ارسالی قرار دهید.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File Paths:\n",
      "['TeaOverflowError.ipynb', 'submission.csv']\n"
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "if not os.path.exists(os.path.join(os.getcwd(), 'TeaOverflowError.ipynb')):\n",
    "    %notebook -e TeaOverflowError.ipynb\n",
    "\n",
    "def compress(file_names):\n",
    "    print(\"File Paths:\")\n",
    "    print(file_names)\n",
    "    compression = zipfile.ZIP_DEFLATED\n",
    "    with zipfile.ZipFile(\"result.zip\", mode=\"w\") as zf:\n",
    "        for file_name in file_names:\n",
    "            zf.write('./' + file_name, file_name, compress_type=compression)\n",
    "\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "file_names = ['TeaOverflowError.ipynb', 'submission.csv']\n",
    "compress(file_names)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
