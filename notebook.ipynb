{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 dir=rtl align=center style=\"line-height:200%;font-family:vazir;color:#0099cc\">\n",
    "<font face=\"vazir\" color=\"#0099cc\">\n",
    "کمک به بحران برق\n",
    "</font>\n",
    "</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 dir=rtl align=right style=\"line-height:200%;font-family:vazir;color:#0099cc\">\n",
    "<font face=\"vazir\" color=\"#0099cc\">\n",
    "مقدمه و صورت مسئله\n",
    "</font>\n",
    "</h2>\n",
    "\n",
    "\n",
    "<p dir=rtl style=\"direction: rtl; text-align: justify; line-height:200%; font-family:vazir; font-size:medium\">\n",
    "<font face=\"vazir\">\n",
    "در این سوال از مسابقه با داده های جمع آوری شده از سنسورهای توربین های بادی سر و کار داریم. با توجه به توضیحات ذکر شده در صفحه سوال داده ها را دانلود کرده و به حل مسئله بپردازید:)\n",
    "</font>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 dir=rtl align=right style=\"line-height:200%;font-family:vazir;color:#0099cc\">\n",
    "<font face=\"vazir\" color=\"#0099cc\">\n",
    "معرفی مجموعه داده\n",
    "</font>\n",
    "</h2>\n",
    "\n",
    "<p dir=rtl style=\"direction: rtl; text-align: justify; line-height:200%; font-family:vazir; font-size:medium\">\n",
    "<font face=\"vazir\" size=3>\n",
    "با استفاده از <a href=\"https://drive.google.com/file/d/162iKuz9w5vuFkG8vbXuTiFswthvL1qZT/view?usp=sharing\">لینک</a> میتوانید به مجموعه داده های آموزش و آزمایش دسترسی داشته باشید و آنها را دانلود کنید.\n",
    "</font> \n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import lightgbm as lgb\n",
    "\n",
    "from scipy.stats import skew, kurtosis # Imported but unused in original, now incorporated for robustness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>vehicle_id</th>\n",
       "      <th>time_step</th>\n",
       "      <th>171_0</th>\n",
       "      <th>666_0</th>\n",
       "      <th>427_0</th>\n",
       "      <th>837_0</th>\n",
       "      <th>167_0</th>\n",
       "      <th>167_1</th>\n",
       "      <th>167_2</th>\n",
       "      <th>167_3</th>\n",
       "      <th>...</th>\n",
       "      <th>397_26</th>\n",
       "      <th>397_27</th>\n",
       "      <th>397_28</th>\n",
       "      <th>397_29</th>\n",
       "      <th>397_30</th>\n",
       "      <th>397_31</th>\n",
       "      <th>397_32</th>\n",
       "      <th>397_33</th>\n",
       "      <th>397_34</th>\n",
       "      <th>397_35</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>4.4</td>\n",
       "      <td>94410.0</td>\n",
       "      <td>7982.0</td>\n",
       "      <td>3115020.0</td>\n",
       "      <td>2625.0</td>\n",
       "      <td>1304.0</td>\n",
       "      <td>366624.0</td>\n",
       "      <td>232040.0</td>\n",
       "      <td>147104.0</td>\n",
       "      <td>...</td>\n",
       "      <td>72548.0</td>\n",
       "      <td>9696.0</td>\n",
       "      <td>1309.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>399.0</td>\n",
       "      <td>31843.0</td>\n",
       "      <td>89138.0</td>\n",
       "      <td>24101.0</td>\n",
       "      <td>5112.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>5.0</td>\n",
       "      <td>105405.0</td>\n",
       "      <td>9178.0</td>\n",
       "      <td>3467240.0</td>\n",
       "      <td>3225.0</td>\n",
       "      <td>1305.0</td>\n",
       "      <td>390896.0</td>\n",
       "      <td>252528.0</td>\n",
       "      <td>157824.0</td>\n",
       "      <td>...</td>\n",
       "      <td>83147.0</td>\n",
       "      <td>12042.0</td>\n",
       "      <td>1841.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>455.0</td>\n",
       "      <td>35749.0</td>\n",
       "      <td>106366.0</td>\n",
       "      <td>28168.0</td>\n",
       "      <td>5420.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>8.2</td>\n",
       "      <td>214440.0</td>\n",
       "      <td>23348.0</td>\n",
       "      <td>7085790.0</td>\n",
       "      <td>8010.0</td>\n",
       "      <td>4897.0</td>\n",
       "      <td>461240.0</td>\n",
       "      <td>378264.0</td>\n",
       "      <td>313016.0</td>\n",
       "      <td>...</td>\n",
       "      <td>198090.0</td>\n",
       "      <td>26099.0</td>\n",
       "      <td>4516.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>1120.0</td>\n",
       "      <td>94017.0</td>\n",
       "      <td>293462.0</td>\n",
       "      <td>85596.0</td>\n",
       "      <td>16655.0</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>9.8</td>\n",
       "      <td>260190.0</td>\n",
       "      <td>29120.0</td>\n",
       "      <td>8696850.0</td>\n",
       "      <td>12525.0</td>\n",
       "      <td>6513.0</td>\n",
       "      <td>493728.0</td>\n",
       "      <td>455352.0</td>\n",
       "      <td>363984.0</td>\n",
       "      <td>...</td>\n",
       "      <td>244326.0</td>\n",
       "      <td>32708.0</td>\n",
       "      <td>5714.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>1498.0</td>\n",
       "      <td>118349.0</td>\n",
       "      <td>378176.0</td>\n",
       "      <td>115130.0</td>\n",
       "      <td>24054.0</td>\n",
       "      <td>35.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>20.6</td>\n",
       "      <td>519780.0</td>\n",
       "      <td>57967.0</td>\n",
       "      <td>17379560.0</td>\n",
       "      <td>32130.0</td>\n",
       "      <td>7617.0</td>\n",
       "      <td>665120.0</td>\n",
       "      <td>818912.0</td>\n",
       "      <td>664784.0</td>\n",
       "      <td>...</td>\n",
       "      <td>500995.0</td>\n",
       "      <td>61122.0</td>\n",
       "      <td>14247.0</td>\n",
       "      <td>169.0</td>\n",
       "      <td>3788.0</td>\n",
       "      <td>222712.0</td>\n",
       "      <td>741413.0</td>\n",
       "      <td>237274.0</td>\n",
       "      <td>59636.0</td>\n",
       "      <td>639.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198135</th>\n",
       "      <td>33638</td>\n",
       "      <td>59.6</td>\n",
       "      <td>1151970.0</td>\n",
       "      <td>53911.0</td>\n",
       "      <td>37965610.0</td>\n",
       "      <td>5760.0</td>\n",
       "      <td>2024.0</td>\n",
       "      <td>1060370.0</td>\n",
       "      <td>1621321.0</td>\n",
       "      <td>1296818.0</td>\n",
       "      <td>...</td>\n",
       "      <td>900593.0</td>\n",
       "      <td>59376.0</td>\n",
       "      <td>8906.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>11783.0</td>\n",
       "      <td>386958.0</td>\n",
       "      <td>717951.0</td>\n",
       "      <td>143395.0</td>\n",
       "      <td>17803.0</td>\n",
       "      <td>127.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198136</th>\n",
       "      <td>33638</td>\n",
       "      <td>65.6</td>\n",
       "      <td>1321905.0</td>\n",
       "      <td>65091.0</td>\n",
       "      <td>43447361.0</td>\n",
       "      <td>6856.0</td>\n",
       "      <td>2024.0</td>\n",
       "      <td>1141978.0</td>\n",
       "      <td>1773113.0</td>\n",
       "      <td>1425914.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1011299.0</td>\n",
       "      <td>69093.0</td>\n",
       "      <td>10390.0</td>\n",
       "      <td>126.0</td>\n",
       "      <td>13428.0</td>\n",
       "      <td>421160.0</td>\n",
       "      <td>812403.0</td>\n",
       "      <td>170877.0</td>\n",
       "      <td>21681.0</td>\n",
       "      <td>141.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198137</th>\n",
       "      <td>33638</td>\n",
       "      <td>69.8</td>\n",
       "      <td>1438665.0</td>\n",
       "      <td>71591.0</td>\n",
       "      <td>47111501.0</td>\n",
       "      <td>7201.0</td>\n",
       "      <td>2024.0</td>\n",
       "      <td>1166667.0</td>\n",
       "      <td>1862506.0</td>\n",
       "      <td>1502034.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1086320.0</td>\n",
       "      <td>73748.0</td>\n",
       "      <td>11454.0</td>\n",
       "      <td>133.0</td>\n",
       "      <td>14184.0</td>\n",
       "      <td>445289.0</td>\n",
       "      <td>880675.0</td>\n",
       "      <td>186809.0</td>\n",
       "      <td>24153.0</td>\n",
       "      <td>148.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198138</th>\n",
       "      <td>33638</td>\n",
       "      <td>70.8</td>\n",
       "      <td>1459155.0</td>\n",
       "      <td>73008.0</td>\n",
       "      <td>47791981.0</td>\n",
       "      <td>7231.0</td>\n",
       "      <td>2024.0</td>\n",
       "      <td>1173795.0</td>\n",
       "      <td>1895626.0</td>\n",
       "      <td>1517987.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1098011.0</td>\n",
       "      <td>74736.0</td>\n",
       "      <td>11672.0</td>\n",
       "      <td>133.0</td>\n",
       "      <td>14436.0</td>\n",
       "      <td>449272.0</td>\n",
       "      <td>890349.0</td>\n",
       "      <td>189582.0</td>\n",
       "      <td>24580.0</td>\n",
       "      <td>148.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198139</th>\n",
       "      <td>33638</td>\n",
       "      <td>71.4</td>\n",
       "      <td>1471830.0</td>\n",
       "      <td>73945.0</td>\n",
       "      <td>48213271.0</td>\n",
       "      <td>7321.0</td>\n",
       "      <td>2024.0</td>\n",
       "      <td>1187164.0</td>\n",
       "      <td>1916914.0</td>\n",
       "      <td>1532811.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1105263.0</td>\n",
       "      <td>75543.0</td>\n",
       "      <td>11814.0</td>\n",
       "      <td>133.0</td>\n",
       "      <td>14605.0</td>\n",
       "      <td>450637.0</td>\n",
       "      <td>896643.0</td>\n",
       "      <td>191990.0</td>\n",
       "      <td>24867.0</td>\n",
       "      <td>162.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>198140 rows × 107 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        vehicle_id  time_step      171_0    666_0       427_0    837_0  \\\n",
       "0                1        4.4    94410.0   7982.0   3115020.0   2625.0   \n",
       "1                1        5.0   105405.0   9178.0   3467240.0   3225.0   \n",
       "2                1        8.2   214440.0  23348.0   7085790.0   8010.0   \n",
       "3                1        9.8   260190.0  29120.0   8696850.0  12525.0   \n",
       "4                1       20.6   519780.0  57967.0  17379560.0  32130.0   \n",
       "...            ...        ...        ...      ...         ...      ...   \n",
       "198135       33638       59.6  1151970.0  53911.0  37965610.0   5760.0   \n",
       "198136       33638       65.6  1321905.0  65091.0  43447361.0   6856.0   \n",
       "198137       33638       69.8  1438665.0  71591.0  47111501.0   7201.0   \n",
       "198138       33638       70.8  1459155.0  73008.0  47791981.0   7231.0   \n",
       "198139       33638       71.4  1471830.0  73945.0  48213271.0   7321.0   \n",
       "\n",
       "         167_0      167_1      167_2      167_3  ...     397_26   397_27  \\\n",
       "0       1304.0   366624.0   232040.0   147104.0  ...    72548.0   9696.0   \n",
       "1       1305.0   390896.0   252528.0   157824.0  ...    83147.0  12042.0   \n",
       "2       4897.0   461240.0   378264.0   313016.0  ...   198090.0  26099.0   \n",
       "3       6513.0   493728.0   455352.0   363984.0  ...   244326.0  32708.0   \n",
       "4       7617.0   665120.0   818912.0   664784.0  ...   500995.0  61122.0   \n",
       "...        ...        ...        ...        ...  ...        ...      ...   \n",
       "198135  2024.0  1060370.0  1621321.0  1296818.0  ...   900593.0  59376.0   \n",
       "198136  2024.0  1141978.0  1773113.0  1425914.0  ...  1011299.0  69093.0   \n",
       "198137  2024.0  1166667.0  1862506.0  1502034.0  ...  1086320.0  73748.0   \n",
       "198138  2024.0  1173795.0  1895626.0  1517987.0  ...  1098011.0  74736.0   \n",
       "198139  2024.0  1187164.0  1916914.0  1532811.0  ...  1105263.0  75543.0   \n",
       "\n",
       "         397_28  397_29   397_30    397_31    397_32    397_33   397_34  \\\n",
       "0        1309.0     7.0    399.0   31843.0   89138.0   24101.0   5112.0   \n",
       "1        1841.0     7.0    455.0   35749.0  106366.0   28168.0   5420.0   \n",
       "2        4516.0    21.0   1120.0   94017.0  293462.0   85596.0  16655.0   \n",
       "3        5714.0    49.0   1498.0  118349.0  378176.0  115130.0  24054.0   \n",
       "4       14247.0   169.0   3788.0  222712.0  741413.0  237274.0  59636.0   \n",
       "...         ...     ...      ...       ...       ...       ...      ...   \n",
       "198135   8906.0    70.0  11783.0  386958.0  717951.0  143395.0  17803.0   \n",
       "198136  10390.0   126.0  13428.0  421160.0  812403.0  170877.0  21681.0   \n",
       "198137  11454.0   133.0  14184.0  445289.0  880675.0  186809.0  24153.0   \n",
       "198138  11672.0   133.0  14436.0  449272.0  890349.0  189582.0  24580.0   \n",
       "198139  11814.0   133.0  14605.0  450637.0  896643.0  191990.0  24867.0   \n",
       "\n",
       "        397_35  \n",
       "0          7.0  \n",
       "1          7.0  \n",
       "2         14.0  \n",
       "3         35.0  \n",
       "4        639.0  \n",
       "...        ...  \n",
       "198135   127.0  \n",
       "198136   141.0  \n",
       "198137   148.0  \n",
       "198138   148.0  \n",
       "198139   162.0  \n",
       "\n",
       "[198140 rows x 107 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\"\"\"\n",
    "AeroGen Dynamics - Predictive Maintenance\n",
    "*** ENHANCED AND SCALABLE SOLUTION ***\n",
    "\n",
    "Optimized script with:\n",
    "1. Robust Null Value Handling (Missing Indicator Flags).\n",
    "2. Advanced Feature Engineering (Statistical moments, Trend, Volatility).\n",
    "3. Cost-Sensitive Learning (LightGBM with custom weights).\n",
    "4. Time-Series Integrity (Mitigation of Label Leakage risks).\n",
    "5. PySpark Placeholder (For true big data scalability, commented out).\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import lightgbm as lgb\n",
    "from scipy.stats import skew, kurtosis # Imported but unused in original, now incorporated for robustness\n",
    "\n",
    "# --- Global Constants ---\n",
    "RANDOM_STATE = 42\n",
    "# The COST_MATRIX defines the business penalty/reward for classification\n",
    "COST_MATRIX = np.array([\n",
    "    [0, -2, -4, -8, -12],\n",
    "    [-15, 20, -3, -6, -10],\n",
    "    [-30, -15, 40, -5, -8],\n",
    "    [-50, -30, -15, 80, -5],\n",
    "    [-80, -50, -30, -15, 150]\n",
    "])\n",
    "\n",
    "\n",
    "# --- SCALABILITY NOTE ---\n",
    "# If your dataset is truly 'huge' (10s to 100s of GBs), the Pandas operations below will\n",
    "# crash due to memory or be too slow. You must use a distributed framework like PySpark.\n",
    "# The code below is optimized for Pandas, but here is a placeholder for Spark:\n",
    "\n",
    "# # from pyspark.sql import SparkSession\n",
    "# # spark = SparkSession.builder.appName(\"AeroGen_Maintenance_Spark\").getOrCreate()\n",
    "# # def load_csv_spark(path):\n",
    "# #     return spark.read.csv(path, header=True, inferSchema=True).cache()\n",
    "# # If using Spark, the 'load_csv' and 'feature_engineering' functions below must be rewritten\n",
    "# # using Spark DataFrames and Window functions.\n",
    "\n",
    "\n",
    "def load_csv(path):\n",
    "    \"\"\"Loads CSV with memory optimization by downcasting numerical columns.\"\"\"\n",
    "    print(f\"Loading {path} ...\")\n",
    "    # Read the data, ensuring 'object' dtype is handled\n",
    "    df = pd.read_csv(path)\n",
    "\n",
    "    # Optimize memory usage by downcasting\n",
    "    for col in df.select_dtypes(include=['int64', 'float64']).columns:\n",
    "        if 'time_step' not in col:\n",
    "            # Use 'float32' for large datasets\n",
    "            df[col] = pd.to_numeric(df[col], downcast='float')\n",
    "\n",
    "    return df\n",
    "\n",
    "# Load Data\n",
    "\n",
    "# NOTE: Ensure these files exist in the execution environment\n",
    "test_op =  pd.read_csv('test_operational_readouts.csv')\n",
    "test_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_8464\\1428763140.py:1: DtypeWarning: Columns (0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,67,68,69,70,72,73,74,75,76,77,79,80,81,82,84,86,89,97,99,103) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  train_op =  pd.read_csv('train_operational_readouts.csv', on_bad_lines='skip' )\n"
     ]
    }
   ],
   "source": [
    "train_op =  pd.read_csv('train_operational_readouts.csv', on_bad_lines='skip' )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "train_specs =  pd.read_csv('train_specifications.csv')\n",
    "train_tte = pd.read_csv('train_tte.csv')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_op =  pd.read_csv('validation_operational_readouts.csv')\n",
    "val_specs =  pd.read_csv('validation_specifications.csv')\n",
    "\n",
    "val_labels =  pd.read_csv('validation_labels.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Feature Engineering...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_8464\\191300169.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'hist_{prefix}_max'] = df[cols].max(axis=1)\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_8464\\191300169.py:64: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'hist_{prefix}_std'] = df[cols].std(axis=1)\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_8464\\191300169.py:65: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'hist_{prefix}_sum'] = df[cols].sum(axis=1)\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_8464\\191300169.py:67: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'hist_{prefix}_min'] = df[cols].min(axis=1)\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_8464\\191300169.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'hist_{prefix}_max'] = df[cols].max(axis=1)\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_8464\\191300169.py:64: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'hist_{prefix}_std'] = df[cols].std(axis=1)\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_8464\\191300169.py:65: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'hist_{prefix}_sum'] = df[cols].sum(axis=1)\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_8464\\191300169.py:67: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'hist_{prefix}_min'] = df[cols].min(axis=1)\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_8464\\191300169.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'hist_{prefix}_max'] = df[cols].max(axis=1)\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_8464\\191300169.py:64: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'hist_{prefix}_std'] = df[cols].std(axis=1)\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_8464\\191300169.py:65: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'hist_{prefix}_sum'] = df[cols].sum(axis=1)\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_8464\\191300169.py:67: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'hist_{prefix}_min'] = df[cols].min(axis=1)\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_8464\\191300169.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'hist_{prefix}_max'] = df[cols].max(axis=1)\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_8464\\191300169.py:64: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'hist_{prefix}_std'] = df[cols].std(axis=1)\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_8464\\191300169.py:65: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'hist_{prefix}_sum'] = df[cols].sum(axis=1)\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_8464\\191300169.py:67: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'hist_{prefix}_min'] = df[cols].min(axis=1)\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_8464\\191300169.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'hist_{prefix}_max'] = df[cols].max(axis=1)\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_8464\\191300169.py:64: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'hist_{prefix}_std'] = df[cols].std(axis=1)\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_8464\\191300169.py:65: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'hist_{prefix}_sum'] = df[cols].sum(axis=1)\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_8464\\191300169.py:67: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'hist_{prefix}_min'] = df[cols].min(axis=1)\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_8464\\191300169.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'hist_{prefix}_max'] = df[cols].max(axis=1)\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_8464\\191300169.py:64: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'hist_{prefix}_std'] = df[cols].std(axis=1)\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_8464\\191300169.py:65: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'hist_{prefix}_sum'] = df[cols].sum(axis=1)\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_8464\\191300169.py:67: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'hist_{prefix}_min'] = df[cols].min(axis=1)\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_8464\\191300169.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'hist_{prefix}_max'] = df[cols].max(axis=1)\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_8464\\191300169.py:64: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'hist_{prefix}_std'] = df[cols].std(axis=1)\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_8464\\191300169.py:65: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'hist_{prefix}_sum'] = df[cols].sum(axis=1)\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_8464\\191300169.py:67: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'hist_{prefix}_min'] = df[cols].min(axis=1)\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_8464\\191300169.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'hist_{prefix}_max'] = df[cols].max(axis=1)\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_8464\\191300169.py:64: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'hist_{prefix}_std'] = df[cols].std(axis=1)\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_8464\\191300169.py:65: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'hist_{prefix}_sum'] = df[cols].sum(axis=1)\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_8464\\191300169.py:67: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'hist_{prefix}_min'] = df[cols].min(axis=1)\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_8464\\191300169.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'hist_{prefix}_max'] = df[cols].max(axis=1)\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_8464\\191300169.py:64: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'hist_{prefix}_std'] = df[cols].std(axis=1)\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_8464\\191300169.py:65: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'hist_{prefix}_sum'] = df[cols].sum(axis=1)\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_8464\\191300169.py:67: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'hist_{prefix}_min'] = df[cols].min(axis=1)\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_8464\\191300169.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'hist_{prefix}_max'] = df[cols].max(axis=1)\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_8464\\191300169.py:64: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'hist_{prefix}_std'] = df[cols].std(axis=1)\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_8464\\191300169.py:65: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'hist_{prefix}_sum'] = df[cols].sum(axis=1)\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_8464\\191300169.py:67: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'hist_{prefix}_min'] = df[cols].min(axis=1)\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_8464\\191300169.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'hist_{prefix}_max'] = df[cols].max(axis=1)\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_8464\\191300169.py:64: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'hist_{prefix}_std'] = df[cols].std(axis=1)\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_8464\\191300169.py:65: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'hist_{prefix}_sum'] = df[cols].sum(axis=1)\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_8464\\191300169.py:67: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'hist_{prefix}_min'] = df[cols].min(axis=1)\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_8464\\191300169.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'hist_{prefix}_max'] = df[cols].max(axis=1)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 105\u001b[39m\n\u001b[32m    103\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m df\n\u001b[32m    104\u001b[39m \u001b[38;5;66;03m# Apply feature engineering to all datasets\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m105\u001b[39m train_full = \u001b[43mfeature_engineering\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_full\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    106\u001b[39m val_op = feature_engineering(val_op.copy())\n\u001b[32m    107\u001b[39m test_op = feature_engineering(test_op.copy())\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 97\u001b[39m, in \u001b[36mfeature_engineering\u001b[39m\u001b[34m(df)\u001b[39m\n\u001b[32m     93\u001b[39m \u001b[38;5;66;03m# New: Statistical Moments (Skewness and Kurtosis) - Captures shape of distribution over time\u001b[39;00m\n\u001b[32m     94\u001b[39m \u001b[38;5;66;03m# Skewness: Measures the asymmetry of the data distribution (critical for anomaly detection)\u001b[39;00m\n\u001b[32m     95\u001b[39m \u001b[38;5;66;03m# Kurtosis: Measures the \"tailedness\" of the distribution (critical for large/outlier events)\u001b[39;00m\n\u001b[32m     96\u001b[39m df[\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcol\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_roll\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mROLLING_WINDOW\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_skew\u001b[39m\u001b[33m'\u001b[39m] = df.groupby(\u001b[33m'\u001b[39m\u001b[33mvehicle_id\u001b[39m\u001b[33m'\u001b[39m)[col].transform(\u001b[38;5;28;01mlambda\u001b[39;00m x: x.rolling(ROLLING_WINDOW, min_periods=ROLLING_WINDOW//\u001b[32m2\u001b[39m).apply(\u001b[38;5;28;01mlambda\u001b[39;00m y: skew(y, nan_policy=\u001b[33m'\u001b[39m\u001b[33momit\u001b[39m\u001b[33m'\u001b[39m), raw=\u001b[38;5;28;01mTrue\u001b[39;00m))\n\u001b[32m---> \u001b[39m\u001b[32m97\u001b[39m df[\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcol\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_roll\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mROLLING_WINDOW\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_kurt\u001b[39m\u001b[33m'\u001b[39m] = \u001b[43mdf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgroupby\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mvehicle_id\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcol\u001b[49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrolling\u001b[49m\u001b[43m(\u001b[49m\u001b[43mROLLING_WINDOW\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmin_periods\u001b[49m\u001b[43m=\u001b[49m\u001b[43mROLLING_WINDOW\u001b[49m\u001b[43m/\u001b[49m\u001b[43m/\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mkurtosis\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnan_policy\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43momit\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mraw\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     99\u001b[39m \u001b[38;5;66;03m# New: Mean/Std Ratio (A normalized measure of volatility)\u001b[39;00m\n\u001b[32m    100\u001b[39m \u001b[38;5;66;03m# Handle division by zero for stability\u001b[39;00m\n\u001b[32m    101\u001b[39m df[\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcol\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_volatility_ratio\u001b[39m\u001b[33m'\u001b[39m] = np.where(roll_std != \u001b[32m0\u001b[39m, roll_mean / roll_std, \u001b[32m0\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\Lib\\site-packages\\pandas\\core\\groupby\\generic.py:517\u001b[39m, in \u001b[36mSeriesGroupBy.transform\u001b[39m\u001b[34m(self, func, engine, engine_kwargs, *args, **kwargs)\u001b[39m\n\u001b[32m    514\u001b[39m \u001b[38;5;129m@Substitution\u001b[39m(klass=\u001b[33m\"\u001b[39m\u001b[33mSeries\u001b[39m\u001b[33m\"\u001b[39m, example=__examples_series_doc)\n\u001b[32m    515\u001b[39m \u001b[38;5;129m@Appender\u001b[39m(_transform_template)\n\u001b[32m    516\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mtransform\u001b[39m(\u001b[38;5;28mself\u001b[39m, func, *args, engine=\u001b[38;5;28;01mNone\u001b[39;00m, engine_kwargs=\u001b[38;5;28;01mNone\u001b[39;00m, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m517\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    518\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mengine\u001b[49m\u001b[43m=\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mengine_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mengine_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    519\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\Lib\\site-packages\\pandas\\core\\groupby\\groupby.py:2027\u001b[39m, in \u001b[36mGroupBy._transform\u001b[39m\u001b[34m(self, func, engine, engine_kwargs, *args, **kwargs)\u001b[39m\n\u001b[32m   2024\u001b[39m     warn_alias_replacement(\u001b[38;5;28mself\u001b[39m, orig_func, func)\n\u001b[32m   2026\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m2027\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_transform_general\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mengine_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2029\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m func \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m base.transform_kernel_allowlist:\n\u001b[32m   2030\u001b[39m     msg = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m is not a valid function name for transform(name)\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\Lib\\site-packages\\pandas\\core\\groupby\\generic.py:557\u001b[39m, in \u001b[36mSeriesGroupBy._transform_general\u001b[39m\u001b[34m(self, func, engine, engine_kwargs, *args, **kwargs)\u001b[39m\n\u001b[32m    552\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m name, group \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._grouper.get_iterator(\n\u001b[32m    553\u001b[39m     \u001b[38;5;28mself\u001b[39m._obj_with_exclusions, axis=\u001b[38;5;28mself\u001b[39m.axis\n\u001b[32m    554\u001b[39m ):\n\u001b[32m    555\u001b[39m     \u001b[38;5;66;03m# this setattr is needed for test_transform_lambda_with_datetimetz\u001b[39;00m\n\u001b[32m    556\u001b[39m     \u001b[38;5;28mobject\u001b[39m.\u001b[34m__setattr__\u001b[39m(group, \u001b[33m\"\u001b[39m\u001b[33mname\u001b[39m\u001b[33m\"\u001b[39m, name)\n\u001b[32m--> \u001b[39m\u001b[32m557\u001b[39m     res = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    559\u001b[39m     results.append(klass(res, index=group.index))\n\u001b[32m    561\u001b[39m \u001b[38;5;66;03m# check for empty \"results\" to avoid concat ValueError\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 97\u001b[39m, in \u001b[36mfeature_engineering.<locals>.<lambda>\u001b[39m\u001b[34m(x)\u001b[39m\n\u001b[32m     93\u001b[39m \u001b[38;5;66;03m# New: Statistical Moments (Skewness and Kurtosis) - Captures shape of distribution over time\u001b[39;00m\n\u001b[32m     94\u001b[39m \u001b[38;5;66;03m# Skewness: Measures the asymmetry of the data distribution (critical for anomaly detection)\u001b[39;00m\n\u001b[32m     95\u001b[39m \u001b[38;5;66;03m# Kurtosis: Measures the \"tailedness\" of the distribution (critical for large/outlier events)\u001b[39;00m\n\u001b[32m     96\u001b[39m df[\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcol\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_roll\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mROLLING_WINDOW\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_skew\u001b[39m\u001b[33m'\u001b[39m] = df.groupby(\u001b[33m'\u001b[39m\u001b[33mvehicle_id\u001b[39m\u001b[33m'\u001b[39m)[col].transform(\u001b[38;5;28;01mlambda\u001b[39;00m x: x.rolling(ROLLING_WINDOW, min_periods=ROLLING_WINDOW//\u001b[32m2\u001b[39m).apply(\u001b[38;5;28;01mlambda\u001b[39;00m y: skew(y, nan_policy=\u001b[33m'\u001b[39m\u001b[33momit\u001b[39m\u001b[33m'\u001b[39m), raw=\u001b[38;5;28;01mTrue\u001b[39;00m))\n\u001b[32m---> \u001b[39m\u001b[32m97\u001b[39m df[\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcol\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_roll\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mROLLING_WINDOW\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_kurt\u001b[39m\u001b[33m'\u001b[39m] = df.groupby(\u001b[33m'\u001b[39m\u001b[33mvehicle_id\u001b[39m\u001b[33m'\u001b[39m)[col].transform(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[43mx\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrolling\u001b[49m\u001b[43m(\u001b[49m\u001b[43mROLLING_WINDOW\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmin_periods\u001b[49m\u001b[43m=\u001b[49m\u001b[43mROLLING_WINDOW\u001b[49m\u001b[43m/\u001b[49m\u001b[43m/\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mkurtosis\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnan_policy\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43momit\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mraw\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m)\n\u001b[32m     99\u001b[39m \u001b[38;5;66;03m# New: Mean/Std Ratio (A normalized measure of volatility)\u001b[39;00m\n\u001b[32m    100\u001b[39m \u001b[38;5;66;03m# Handle division by zero for stability\u001b[39;00m\n\u001b[32m    101\u001b[39m df[\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcol\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_volatility_ratio\u001b[39m\u001b[33m'\u001b[39m] = np.where(roll_std != \u001b[32m0\u001b[39m, roll_mean / roll_std, \u001b[32m0\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\Lib\\site-packages\\pandas\\core\\window\\rolling.py:2049\u001b[39m, in \u001b[36mRolling.apply\u001b[39m\u001b[34m(self, func, raw, engine, engine_kwargs, args, kwargs)\u001b[39m\n\u001b[32m   2016\u001b[39m \u001b[38;5;129m@doc\u001b[39m(\n\u001b[32m   2017\u001b[39m     template_header,\n\u001b[32m   2018\u001b[39m     create_section_header(\u001b[33m\"\u001b[39m\u001b[33mParameters\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m   (...)\u001b[39m\u001b[32m   2047\u001b[39m     kwargs: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   2048\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m2049\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2050\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2051\u001b[39m \u001b[43m        \u001b[49m\u001b[43mraw\u001b[49m\u001b[43m=\u001b[49m\u001b[43mraw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2052\u001b[39m \u001b[43m        \u001b[49m\u001b[43mengine\u001b[49m\u001b[43m=\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2053\u001b[39m \u001b[43m        \u001b[49m\u001b[43mengine_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mengine_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2054\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2055\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2056\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\Lib\\site-packages\\pandas\\core\\window\\rolling.py:1508\u001b[39m, in \u001b[36mRollingAndExpandingMixin.apply\u001b[39m\u001b[34m(self, func, raw, engine, engine_kwargs, args, kwargs)\u001b[39m\n\u001b[32m   1505\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1506\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mengine must be either \u001b[39m\u001b[33m'\u001b[39m\u001b[33mnumba\u001b[39m\u001b[33m'\u001b[39m\u001b[33m or \u001b[39m\u001b[33m'\u001b[39m\u001b[33mcython\u001b[39m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1508\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1509\u001b[39m \u001b[43m    \u001b[49m\u001b[43mapply_func\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1510\u001b[39m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mapply\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1511\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnumba_args\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnumba_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1512\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\Lib\\site-packages\\pandas\\core\\window\\rolling.py:619\u001b[39m, in \u001b[36mBaseWindow._apply\u001b[39m\u001b[34m(self, func, name, numeric_only, numba_args, **kwargs)\u001b[39m\n\u001b[32m    616\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n\u001b[32m    618\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.method == \u001b[33m\"\u001b[39m\u001b[33msingle\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m619\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_apply_columnwise\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhomogeneous_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumeric_only\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    620\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    621\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._apply_tablewise(homogeneous_func, name, numeric_only)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\Lib\\site-packages\\pandas\\core\\window\\rolling.py:472\u001b[39m, in \u001b[36mBaseWindow._apply_columnwise\u001b[39m\u001b[34m(self, homogeneous_func, name, numeric_only)\u001b[39m\n\u001b[32m    470\u001b[39m \u001b[38;5;28mself\u001b[39m._validate_numeric_only(name, numeric_only)\n\u001b[32m    471\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._selected_obj.ndim == \u001b[32m1\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m472\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_apply_series\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhomogeneous_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    474\u001b[39m obj = \u001b[38;5;28mself\u001b[39m._create_data(\u001b[38;5;28mself\u001b[39m._selected_obj, numeric_only)\n\u001b[32m    475\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m name == \u001b[33m\"\u001b[39m\u001b[33mcount\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    476\u001b[39m     \u001b[38;5;66;03m# GH 12541: Special case for count where we support date-like types\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\Lib\\site-packages\\pandas\\core\\window\\rolling.py:456\u001b[39m, in \u001b[36mBaseWindow._apply_series\u001b[39m\u001b[34m(self, homogeneous_func, name)\u001b[39m\n\u001b[32m    453\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mTypeError\u001b[39;00m, \u001b[38;5;167;01mNotImplementedError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m    454\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m DataError(\u001b[33m\"\u001b[39m\u001b[33mNo numeric types to aggregate\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m456\u001b[39m result = \u001b[43mhomogeneous_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    457\u001b[39m index = \u001b[38;5;28mself\u001b[39m._slice_axis_for_step(obj.index, result)\n\u001b[32m    458\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m obj._constructor(result, index=index, name=obj.name)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\Lib\\site-packages\\pandas\\core\\window\\rolling.py:614\u001b[39m, in \u001b[36mBaseWindow._apply.<locals>.homogeneous_func\u001b[39m\u001b[34m(values)\u001b[39m\n\u001b[32m    611\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m func(x, start, end, min_periods, *numba_args)\n\u001b[32m    613\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m np.errstate(\u001b[38;5;28mall\u001b[39m=\u001b[33m\"\u001b[39m\u001b[33mignore\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m614\u001b[39m     result = \u001b[43mcalc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    616\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\Lib\\site-packages\\pandas\\core\\window\\rolling.py:611\u001b[39m, in \u001b[36mBaseWindow._apply.<locals>.homogeneous_func.<locals>.calc\u001b[39m\u001b[34m(x)\u001b[39m\n\u001b[32m    602\u001b[39m start, end = window_indexer.get_window_bounds(\n\u001b[32m    603\u001b[39m     num_values=\u001b[38;5;28mlen\u001b[39m(x),\n\u001b[32m    604\u001b[39m     min_periods=min_periods,\n\u001b[32m   (...)\u001b[39m\u001b[32m    607\u001b[39m     step=\u001b[38;5;28mself\u001b[39m.step,\n\u001b[32m    608\u001b[39m )\n\u001b[32m    609\u001b[39m \u001b[38;5;28mself\u001b[39m._check_window_bounds(start, end, \u001b[38;5;28mlen\u001b[39m(x))\n\u001b[32m--> \u001b[39m\u001b[32m611\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mend\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmin_periods\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43mnumba_args\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\Lib\\site-packages\\pandas\\core\\window\\rolling.py:1535\u001b[39m, in \u001b[36mRollingAndExpandingMixin._generate_cython_apply_func.<locals>.apply_func\u001b[39m\u001b[34m(values, begin, end, min_periods, raw)\u001b[39m\n\u001b[32m   1532\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m raw:\n\u001b[32m   1533\u001b[39m     \u001b[38;5;66;03m# GH 45912\u001b[39;00m\n\u001b[32m   1534\u001b[39m     values = Series(values, index=\u001b[38;5;28mself\u001b[39m._on, copy=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m-> \u001b[39m\u001b[32m1535\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwindow_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbegin\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mend\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmin_periods\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/window/aggregations.pyx:1420\u001b[39m, in \u001b[36mpandas._libs.window.aggregations.roll_apply\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 97\u001b[39m, in \u001b[36mfeature_engineering.<locals>.<lambda>\u001b[39m\u001b[34m(y)\u001b[39m\n\u001b[32m     93\u001b[39m \u001b[38;5;66;03m# New: Statistical Moments (Skewness and Kurtosis) - Captures shape of distribution over time\u001b[39;00m\n\u001b[32m     94\u001b[39m \u001b[38;5;66;03m# Skewness: Measures the asymmetry of the data distribution (critical for anomaly detection)\u001b[39;00m\n\u001b[32m     95\u001b[39m \u001b[38;5;66;03m# Kurtosis: Measures the \"tailedness\" of the distribution (critical for large/outlier events)\u001b[39;00m\n\u001b[32m     96\u001b[39m df[\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcol\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_roll\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mROLLING_WINDOW\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_skew\u001b[39m\u001b[33m'\u001b[39m] = df.groupby(\u001b[33m'\u001b[39m\u001b[33mvehicle_id\u001b[39m\u001b[33m'\u001b[39m)[col].transform(\u001b[38;5;28;01mlambda\u001b[39;00m x: x.rolling(ROLLING_WINDOW, min_periods=ROLLING_WINDOW//\u001b[32m2\u001b[39m).apply(\u001b[38;5;28;01mlambda\u001b[39;00m y: skew(y, nan_policy=\u001b[33m'\u001b[39m\u001b[33momit\u001b[39m\u001b[33m'\u001b[39m), raw=\u001b[38;5;28;01mTrue\u001b[39;00m))\n\u001b[32m---> \u001b[39m\u001b[32m97\u001b[39m df[\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcol\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_roll\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mROLLING_WINDOW\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_kurt\u001b[39m\u001b[33m'\u001b[39m] = df.groupby(\u001b[33m'\u001b[39m\u001b[33mvehicle_id\u001b[39m\u001b[33m'\u001b[39m)[col].transform(\u001b[38;5;28;01mlambda\u001b[39;00m x: x.rolling(ROLLING_WINDOW, min_periods=ROLLING_WINDOW//\u001b[32m2\u001b[39m).apply(\u001b[38;5;28;01mlambda\u001b[39;00m y: \u001b[43mkurtosis\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnan_policy\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43momit\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m, raw=\u001b[38;5;28;01mTrue\u001b[39;00m))\n\u001b[32m     99\u001b[39m \u001b[38;5;66;03m# New: Mean/Std Ratio (A normalized measure of volatility)\u001b[39;00m\n\u001b[32m    100\u001b[39m \u001b[38;5;66;03m# Handle division by zero for stability\u001b[39;00m\n\u001b[32m    101\u001b[39m df[\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcol\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_volatility_ratio\u001b[39m\u001b[33m'\u001b[39m] = np.where(roll_std != \u001b[32m0\u001b[39m, roll_mean / roll_std, \u001b[32m0\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\Lib\\site-packages\\scipy\\stats\\_axis_nan_policy.py:528\u001b[39m, in \u001b[36m_axis_nan_policy_factory.<locals>.axis_nan_policy_decorator.<locals>.axis_nan_policy_wrapper\u001b[39m\u001b[34m(***failed resolving arguments***)\u001b[39m\n\u001b[32m    525\u001b[39m     samples = [np.asarray(sample.ravel()) \u001b[38;5;28;01mfor\u001b[39;00m sample \u001b[38;5;129;01min\u001b[39;00m samples]\n\u001b[32m    526\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    527\u001b[39m     \u001b[38;5;66;03m# don't ignore any axes when broadcasting if paired\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m528\u001b[39m     samples = \u001b[43m_broadcast_arrays\u001b[49m\u001b[43m(\u001b[49m\u001b[43msamples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpaired\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    529\u001b[39m     axis = np.atleast_1d(axis)\n\u001b[32m    530\u001b[39m     n_axes = \u001b[38;5;28mlen\u001b[39m(axis)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\Lib\\site-packages\\scipy\\stats\\_axis_nan_policy.py:51\u001b[39m, in \u001b[36m_broadcast_arrays\u001b[39m\u001b[34m(arrays, axis, xp)\u001b[39m\n\u001b[32m     49\u001b[39m arrays = [xp.asarray(arr) \u001b[38;5;28;01mfor\u001b[39;00m arr \u001b[38;5;129;01min\u001b[39;00m arrays]\n\u001b[32m     50\u001b[39m shapes = [arr.shape \u001b[38;5;28;01mfor\u001b[39;00m arr \u001b[38;5;129;01min\u001b[39;00m arrays]\n\u001b[32m---> \u001b[39m\u001b[32m51\u001b[39m new_shapes = \u001b[43m_broadcast_shapes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshapes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     52\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m axis \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     53\u001b[39m     new_shapes = [new_shapes]*\u001b[38;5;28mlen\u001b[39m(arrays)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\Lib\\site-packages\\scipy\\stats\\_axis_nan_policy.py:97\u001b[39m, in \u001b[36m_broadcast_shapes\u001b[39m\u001b[34m(shapes, axis)\u001b[39m\n\u001b[32m     94\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m AxisError(\u001b[33m\"\u001b[39m\u001b[33m`axis` must contain only distinct elements\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     96\u001b[39m     removed_shapes = new_shapes[:, axis]\n\u001b[32m---> \u001b[39m\u001b[32m97\u001b[39m     new_shapes = \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdelete\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_shapes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     99\u001b[39m \u001b[38;5;66;03m# If arrays are broadcastable, shape elements that are 1 may be replaced\u001b[39;00m\n\u001b[32m    100\u001b[39m \u001b[38;5;66;03m# with a corresponding non-1 shape element. Assuming arrays are\u001b[39;00m\n\u001b[32m    101\u001b[39m \u001b[38;5;66;03m# broadcastable, that final shape element can be found with:\u001b[39;00m\n\u001b[32m    102\u001b[39m new_shape = np.max(new_shapes, axis=\u001b[32m0\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\Lib\\site-packages\\numpy\\lib\\_function_base_impl.py:5285\u001b[39m, in \u001b[36m_delete_dispatcher\u001b[39m\u001b[34m(arr, obj, axis)\u001b[39m\n\u001b[32m   5280\u001b[39m         output = \u001b[38;5;28mtuple\u001b[39m(x.copy() \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m output)\n\u001b[32m   5282\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m output\n\u001b[32m-> \u001b[39m\u001b[32m5285\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_delete_dispatcher\u001b[39m(arr, obj, axis=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m   5286\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m (arr, obj)\n\u001b[32m   5289\u001b[39m \u001b[38;5;129m@array_function_dispatch\u001b[39m(_delete_dispatcher)\n\u001b[32m   5290\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdelete\u001b[39m(arr, obj, axis=\u001b[38;5;28;01mNone\u001b[39;00m):\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# --- 1. Label Generation (Preventing Label Leakage / Temporal Bias) ---\n",
    "# Your original function for mapping Time-To-Failure (TTF) to classes\n",
    "def ttf_to_class(ttf):\n",
    "    \"\"\"Maps Time-To-Failure (hours) to a categorical maintenance class (0-4).\"\"\"\n",
    "    if ttf > 48:\n",
    "        return 0\n",
    "    elif 24 < ttf <= 48:\n",
    "        return 1\n",
    "    elif 12 < ttf <= 24:\n",
    "        return 2\n",
    "    elif 6 < ttf <= 12:\n",
    "\n",
    "        return 3\n",
    "    else:\n",
    "        return 4\n",
    "    \n",
    "# Merge TTE data onto the operational data\n",
    "train_full = train_op.merge(train_tte, on='vehicle_id', how='left')\n",
    "\n",
    "# Calculate the actual TTF for every time step *leading up* to the failure.\n",
    "# This is a critical step to ensure every row has a label corresponding to its future state.\n",
    "# For healthy (in_study_repair=0) or post-repair data, use a large placeholder.\n",
    "train_full['time_to_failure'] = np.where(\n",
    "    train_full['in_study_repair'] == 1,\n",
    "    train_full['length_of_study_time_step'] - train_full['time_step'],\n",
    "    999999 # Placeholder for healthy turbines (Class 0)\n",
    ")\n",
    "train_full['class_label'] = train_full['time_to_failure'].apply(ttf_to_class)\n",
    "\n",
    "# --- 2. Feature Engineering (Enhanced Dimensionality Reduction and Stat Moments) ---\n",
    "\n",
    "# Identify histogram columns (columns with the same numerical prefix)\n",
    "def get_hist_cols(df):\n",
    "    \"\"\"Identifies columns that belong to aggregated histogram features.\"\"\"\n",
    "    hist_cols = {}\n",
    "    for col in df.columns:\n",
    "        if '_' in col:\n",
    "            prefix = col.split('_')[0]\n",
    "            # Assumes sensor columns are prefixed by numbers if they are histogram bins\n",
    "            if prefix.isdigit():\n",
    "                if prefix not in hist_cols:\n",
    "                    hist_cols[prefix] = []\n",
    "                hist_cols[prefix].append(col)\n",
    "    return hist_cols\n",
    "\n",
    "HISTOGRAM_COLUMNS = [c for v in get_hist_cols(train_full).values() for c in v]\n",
    "NON_SENSOR_COLS = ['vehicle_id', 'time_step', 'length_of_study_time_step', 'in_study_repair', 'time_to_failure', 'class_label']\n",
    "SENSOR_COLS = [c for c in train_full.columns if c not in NON_SENSOR_COLS and c not in HISTOGRAM_COLUMNS]\n",
    "\n",
    "def feature_engineering(df):\n",
    "    print(\"Starting Feature Engineering...\")\n",
    "    object_cols_to_clean = df.select_dtypes(include =['object']).columns\n",
    "    for col_name in object_cols_to_clean :\n",
    "        df [col_name] = df[col_name].astype(str).str.strip()\n",
    "        df [col_name]=pd.to_numeric(df[col_name],errors='coerce')\n",
    "\n",
    "    \n",
    "    for prefix, cols in get_hist_cols(df).items():\n",
    "        if not cols: continue\n",
    "\n",
    "        # Mean/Spread/Total Activity (Original Features)\n",
    "        df[f'hist_{prefix}_std'] = df[cols].std(axis=1)\n",
    "        df[f'hist_{prefix}_sum'] = df[cols].sum(axis=1)\n",
    "        # New: Min/Max to capture range of activity in the histogram\n",
    "        df[f'hist_{prefix}_min'] = df[cols].min(axis=1)\n",
    "        df[f'hist_{prefix}_max'] = df[cols].max(axis=1)\n",
    "\n",
    "\n",
    "    # Drop individual bins after feature extraction to save memory (Crucial for large data)\n",
    "    df.drop(columns=HISTOGRAM_COLUMNS, errors='ignore', inplace=True)\n",
    "      # 2.2. Time Series Features (Lag, Rolling Statistics, and Moments)\n",
    "    lag_roll_cols = SENSOR_COLS + [c for c in df.columns if c.startswith('hist_') and c.endswith(('_mean', '_sum'))]\n",
    "\n",
    "    # Define rolling window size\n",
    "    ROLLING_WINDOW = 10\n",
    "\n",
    "    for col in lag_roll_cols:\n",
    "        # Lag Features (Change since last time step - Captures instantaneous velocity/trend)\n",
    "        df[f'{col}_lag1'] = df.groupby('vehicle_id')[col].diff(1)\n",
    "        df[f'{col}_lag3'] = df.groupby('vehicle_id')[col].diff(3) # Deeper lag\n",
    "        # Rolling Mean and STD (Trend and Stability over the last N steps)\n",
    "        # Using min_periods=1 is important to get values even at the start of a sequence\n",
    "        roll_mean = df.groupby('vehicle_id')[col].transform(lambda x: x.rolling(ROLLING_WINDOW, min_periods=1).mean())\n",
    "        roll_std = df.groupby('vehicle_id')[col].transform(lambda x: x.rolling(ROLLING_WINDOW, min_periods=1).std())\n",
    "        df[f'{col}_roll{ROLLING_WINDOW}_mean'] = roll_mean\n",
    "        df[f'{col}_roll{ROLLING_WINDOW}_std'] = roll_std\n",
    "        # New: Statistical Moments (Skewness and Kurtosis) - Captures shape of distribution over time\n",
    "        # Skewness: Measures the asymmetry of the data distribution (critical for anomaly detection)\n",
    "        # Kurtosis: Measures the \"tailedness\" of the distribution (critical for large/outlier events)\n",
    "        df[f'{col}_roll{ROLLING_WINDOW}_skew'] = df.groupby('vehicle_id')[col].transform(lambda x: x.rolling(ROLLING_WINDOW, min_periods=ROLLING_WINDOW//2).apply(lambda y: skew(y, nan_policy='omit'), raw=True))\n",
    "        df[f'{col}_roll{ROLLING_WINDOW}_kurt'] = df.groupby('vehicle_id')[col].transform(lambda x: x.rolling(ROLLING_WINDOW, min_periods=ROLLING_WINDOW//2).apply(lambda y: kurtosis(y, nan_policy='omit'), raw=True))\n",
    "        # New: Mean/Std Ratio (A normalized measure of volatility)\n",
    "        # Handle division by zero for stability\n",
    "        df[f'{col}_volatility_ratio'] = np.where(roll_std != 0, roll_mean / roll_std, 0)\n",
    "\n",
    "    return df\n",
    "# Apply feature engineering to all datasets\n",
    "train_full = feature_engineering(train_full.copy())\n",
    "val_op = feature_engineering(val_op.copy())\n",
    "test_op = feature_engineering(test_op.copy())\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_8464\\3041010980.py:4: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  val_final = val_op.groupby('vehicle_id' ,group_keys=False).apply(lambda g: g.loc[g['time_step'].idxmax()]).reset_index(drop=True)\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_8464\\3041010980.py:5: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  test_final = test_op.groupby('vehicle_id',group_keys=False).apply(lambda g: g.loc[g['time_step'].idxmax()]).reset_index(drop=True)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- 3. Final Sample Selection (Latest time step only for Val/Test) ---\n",
    "\n",
    "# Select last row for Validation and Test as required by the submission format\n",
    "val_final = val_op.groupby('vehicle_id' ,group_keys=False).apply(lambda g: g.loc[g['time_step'].idxmax()]).reset_index(drop=True)\n",
    "test_final = test_op.groupby('vehicle_id',group_keys=False).apply(lambda g: g.loc[g['time_step'].idxmax()]).reset_index(drop=True)\n",
    "\n",
    "# Select ALL rows for Training to maximize samples for the model\n",
    "train_final = train_full.copy()\n",
    "# Remove initial NaN values created by rolling/lag features, which are usually a small percentage\n",
    "train_final.dropna(subset=['class_label'], inplace=True)\n",
    "\n",
    "# --- 4. Merge Specifications and Preprocessing (Robust Imputation) ---\n",
    "\n",
    "# Merge specifications\n",
    "train_final = train_final.merge(train_specs, on='vehicle_id', how='left')\n",
    "val_final = val_final.merge(val_specs, on='vehicle_id', how='left')\n",
    "# Use train_specs for test data as in original, assuming similar structural components\n",
    "test_final = test_final.merge(train_specs, on='vehicle_id', how='left')\n",
    "\n",
    "# Handle Categorical Columns (Label Encoding is fine for LightGBM)\n",
    "categorical_cols = train_specs.select_dtypes(include=['object']).columns.tolist()\n",
    "for col in categorical_cols:\n",
    "    le = LabelEncoder()\n",
    "    # Combine all data for consistent encoding\n",
    "    full_data = pd.concat([train_final[col], val_final[col], test_final[col]]).astype(str).fillna('missing')\n",
    "    le.fit(full_data)\n",
    "\n",
    "    for df in [train_final, val_final, test_final]:\n",
    "        df[col] = df[col].astype(str).fillna('missing')\n",
    "        df[col] = le.transform(df[col])\n",
    "\n",
    "# Prepare Train/Validation Sets\n",
    "EXCLUDE_COLS = NON_SENSOR_COLS + ['length_of_study_time_step', 'in_study_repair', 'time_to_failure']\n",
    "X_train = train_final.drop(columns=EXCLUDE_COLS, errors='ignore')\n",
    "y_train = train_final['class_label'].astype(int)\n",
    "\n",
    "# Align validation labels\n",
    "val_labels.set_index('vehicle_id', inplace=True)\n",
    "val_final = val_final[val_final['vehicle_id'].isin(val_labels.index)]\n",
    "y_val = val_labels.loc[val_final['vehicle_id']]['class_label'].astype(int)\n",
    "X_val = val_final.drop(columns=['vehicle_id', 'time_step'], errors='ignore')\n",
    "\n",
    "# Align test data\n",
    "X_test = test_final.drop(columns=['vehicle_id', 'time_step'], errors='ignore')\n",
    "\n",
    "# Impute and Scale Numerical Columns (Addressing the 'Null Value' concern)\n",
    "num_cols = X_train.select_dtypes(include=[np.number]).columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Custom Class Weights Applied: [2.04959873e-01 3.40102564e+01 1.03355844e+02 6.57950396e+02\n",
      " 1.11322857e+03]\n",
      "\n",
      "--- Starting LightGBM Training (Cost-Sensitive) ---\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "pandas dtypes must be int, float or bool.\nFields with bad pandas dtypes: 171_0: object, 666_0: object, 427_0: object, 837_0: object, 167_0: object, 167_1: object, 167_2: object, 167_3: object, 167_4: object, 167_5: object, 167_6: object, 167_7: object, 167_8: object, 167_9: object, 309_0: object, 272_0: object, 272_1: object, 272_2: object, 272_3: object, 272_4: object, 272_5: object, 272_6: object, 272_7: object, 272_8: object, 272_9: object, 835_0: object, 370_0: object, 291_0: object, 291_1: object, 291_2: object, 291_3: object, 291_4: object, 291_5: object, 291_6: object, 291_7: object, 291_8: object, 291_9: object, 291_10: object, 158_0: object, 158_1: object, 158_2: object, 158_3: object, 158_4: object, 158_5: object, 158_6: object, 158_7: object, 158_8: object, 158_9: object, 100_0: object, 459_0: object, 459_1: object, 459_2: object, 459_3: object, 459_4: object, 459_5: object, 459_6: object, 459_7: object, 459_8: object, 459_9: object, 459_10: object, 459_11: object, 459_12: object, 459_13: object, 459_14: object, 459_16: object, 459_17: object, 459_18: object, 459_19: object, 397_1: object, 397_2: object, 397_3: object, 397_4: object, 397_5: object, 397_6: object, 397_8: object, 397_9: object, 397_10: object, 397_11: object, 397_13: object, 397_15: object, 397_18: object, 397_26: object, 397_28: object, 397_32: object",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 65\u001b[39m\n\u001b[32m     62\u001b[39m val_data = lgb.Dataset(X_val, label=y_val)\n\u001b[32m     64\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m--- Starting LightGBM Training (Cost-Sensitive) ---\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m65\u001b[39m model = \u001b[43mlgb\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     66\u001b[39m \u001b[43m                  \u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     67\u001b[39m \u001b[43m                  \u001b[49m\u001b[43mvalid_sets\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mval_data\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     68\u001b[39m \u001b[43m                  \u001b[49m\u001b[43mnum_boost_round\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m3000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# Increased rounds\u001b[39;49;00m\n\u001b[32m     69\u001b[39m \u001b[43m                  \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlgb\u001b[49m\u001b[43m.\u001b[49m\u001b[43mearly_stopping\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstopping_rounds\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m150\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     70\u001b[39m \u001b[43m                             \u001b[49m\u001b[43mlgb\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlog_evaluation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mperiod\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m100\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\Lib\\site-packages\\lightgbm\\engine.py:297\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m(params, train_set, num_boost_round, valid_sets, valid_names, feval, init_model, keep_training_booster, callbacks)\u001b[39m\n\u001b[32m    295\u001b[39m \u001b[38;5;66;03m# construct booster\u001b[39;00m\n\u001b[32m    296\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m297\u001b[39m     booster = \u001b[43mBooster\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_set\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_set\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    298\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m is_valid_contain_train:\n\u001b[32m    299\u001b[39m         booster.set_train_data_name(train_data_name)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\Lib\\site-packages\\lightgbm\\basic.py:3656\u001b[39m, in \u001b[36mBooster.__init__\u001b[39m\u001b[34m(self, params, train_set, model_file, model_str)\u001b[39m\n\u001b[32m   3649\u001b[39m     \u001b[38;5;28mself\u001b[39m.set_network(\n\u001b[32m   3650\u001b[39m         machines=machines,\n\u001b[32m   3651\u001b[39m         local_listen_port=params[\u001b[33m\"\u001b[39m\u001b[33mlocal_listen_port\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m   3652\u001b[39m         listen_time_out=params.get(\u001b[33m\"\u001b[39m\u001b[33mtime_out\u001b[39m\u001b[33m\"\u001b[39m, \u001b[32m120\u001b[39m),\n\u001b[32m   3653\u001b[39m         num_machines=params[\u001b[33m\"\u001b[39m\u001b[33mnum_machines\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m   3654\u001b[39m     )\n\u001b[32m   3655\u001b[39m \u001b[38;5;66;03m# construct booster object\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m3656\u001b[39m \u001b[43mtrain_set\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconstruct\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3657\u001b[39m \u001b[38;5;66;03m# copy the parameters from train_set\u001b[39;00m\n\u001b[32m   3658\u001b[39m params.update(train_set.get_params())\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\Lib\\site-packages\\lightgbm\\basic.py:2590\u001b[39m, in \u001b[36mDataset.construct\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   2585\u001b[39m             \u001b[38;5;28mself\u001b[39m._set_init_score_by_predictor(\n\u001b[32m   2586\u001b[39m                 predictor=\u001b[38;5;28mself\u001b[39m._predictor, data=\u001b[38;5;28mself\u001b[39m.data, used_indices=used_indices\n\u001b[32m   2587\u001b[39m             )\n\u001b[32m   2588\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   2589\u001b[39m     \u001b[38;5;66;03m# create train\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2590\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_lazy_init\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2591\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2592\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2593\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreference\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   2594\u001b[39m \u001b[43m        \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2595\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2596\u001b[39m \u001b[43m        \u001b[49m\u001b[43minit_score\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minit_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2597\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpredictor\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_predictor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2598\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfeature_name\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfeature_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2599\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcategorical_feature\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcategorical_feature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2600\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2601\u001b[39m \u001b[43m        \u001b[49m\u001b[43mposition\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mposition\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2602\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2603\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.free_raw_data:\n\u001b[32m   2604\u001b[39m     \u001b[38;5;28mself\u001b[39m.data = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\Lib\\site-packages\\lightgbm\\basic.py:2123\u001b[39m, in \u001b[36mDataset._lazy_init\u001b[39m\u001b[34m(self, data, label, reference, weight, group, init_score, predictor, feature_name, categorical_feature, params, position)\u001b[39m\n\u001b[32m   2121\u001b[39m     categorical_feature = reference.categorical_feature\n\u001b[32m   2122\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, pd_DataFrame):\n\u001b[32m-> \u001b[39m\u001b[32m2123\u001b[39m     data, feature_name, categorical_feature, \u001b[38;5;28mself\u001b[39m.pandas_categorical = \u001b[43m_data_from_pandas\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2124\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2125\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfeature_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfeature_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2126\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcategorical_feature\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcategorical_feature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2127\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpandas_categorical\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpandas_categorical\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2128\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2129\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m _is_pyarrow_table(data) \u001b[38;5;129;01mand\u001b[39;00m feature_name == \u001b[33m\"\u001b[39m\u001b[33mauto\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m   2130\u001b[39m     feature_name = data.column_names\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\Lib\\site-packages\\lightgbm\\basic.py:868\u001b[39m, in \u001b[36m_data_from_pandas\u001b[39m\u001b[34m(data, feature_name, categorical_feature, pandas_categorical)\u001b[39m\n\u001b[32m    864\u001b[39m df_dtypes.append(np.float32)\n\u001b[32m    865\u001b[39m target_dtype = np.result_type(*df_dtypes)\n\u001b[32m    867\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[32m--> \u001b[39m\u001b[32m868\u001b[39m     \u001b[43m_pandas_to_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_dtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtarget_dtype\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[32m    869\u001b[39m     feature_name,\n\u001b[32m    870\u001b[39m     categorical_feature,\n\u001b[32m    871\u001b[39m     pandas_categorical,\n\u001b[32m    872\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\Lib\\site-packages\\lightgbm\\basic.py:814\u001b[39m, in \u001b[36m_pandas_to_numpy\u001b[39m\u001b[34m(data, target_dtype)\u001b[39m\n\u001b[32m    810\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_pandas_to_numpy\u001b[39m(\n\u001b[32m    811\u001b[39m     data: pd_DataFrame,\n\u001b[32m    812\u001b[39m     target_dtype: \u001b[33m\"\u001b[39m\u001b[33mnp.typing.DTypeLike\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    813\u001b[39m ) -> np.ndarray:\n\u001b[32m--> \u001b[39m\u001b[32m814\u001b[39m     \u001b[43m_check_for_bad_pandas_dtypes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdtypes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    815\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    816\u001b[39m         \u001b[38;5;66;03m# most common case (no nullable dtypes)\u001b[39;00m\n\u001b[32m    817\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m data.to_numpy(dtype=target_dtype, copy=\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\Lib\\site-packages\\lightgbm\\basic.py:805\u001b[39m, in \u001b[36m_check_for_bad_pandas_dtypes\u001b[39m\u001b[34m(pandas_dtypes_series)\u001b[39m\n\u001b[32m    799\u001b[39m bad_pandas_dtypes = [\n\u001b[32m    800\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcolumn_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpandas_dtype\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    801\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m column_name, pandas_dtype \u001b[38;5;129;01min\u001b[39;00m pandas_dtypes_series.items()\n\u001b[32m    802\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _is_allowed_numpy_dtype(pandas_dtype.type)\n\u001b[32m    803\u001b[39m ]\n\u001b[32m    804\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m bad_pandas_dtypes:\n\u001b[32m--> \u001b[39m\u001b[32m805\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    806\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mpandas dtypes must be int, float or bool.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mFields with bad pandas dtypes: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m, \u001b[39m\u001b[33m'\u001b[39m.join(bad_pandas_dtypes)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    807\u001b[39m     )\n",
      "\u001b[31mValueError\u001b[39m: pandas dtypes must be int, float or bool.\nFields with bad pandas dtypes: 171_0: object, 666_0: object, 427_0: object, 837_0: object, 167_0: object, 167_1: object, 167_2: object, 167_3: object, 167_4: object, 167_5: object, 167_6: object, 167_7: object, 167_8: object, 167_9: object, 309_0: object, 272_0: object, 272_1: object, 272_2: object, 272_3: object, 272_4: object, 272_5: object, 272_6: object, 272_7: object, 272_8: object, 272_9: object, 835_0: object, 370_0: object, 291_0: object, 291_1: object, 291_2: object, 291_3: object, 291_4: object, 291_5: object, 291_6: object, 291_7: object, 291_8: object, 291_9: object, 291_10: object, 158_0: object, 158_1: object, 158_2: object, 158_3: object, 158_4: object, 158_5: object, 158_6: object, 158_7: object, 158_8: object, 158_9: object, 100_0: object, 459_0: object, 459_1: object, 459_2: object, 459_3: object, 459_4: object, 459_5: object, 459_6: object, 459_7: object, 459_8: object, 459_9: object, 459_10: object, 459_11: object, 459_12: object, 459_13: object, 459_14: object, 459_16: object, 459_17: object, 459_18: object, 459_19: object, 397_1: object, 397_2: object, 397_3: object, 397_4: object, 397_5: object, 397_6: object, 397_8: object, 397_9: object, 397_10: object, 397_11: object, 397_13: object, 397_15: object, 397_18: object, 397_26: object, 397_28: object, 397_32: object"
     ]
    }
   ],
   "source": [
    "\n",
    "for c in num_cols:\n",
    "    median_val = X_train[c].median()\n",
    "\n",
    "    # 1. Add Missing Indicator Flags (Robust Null Handling)\n",
    "    # This is critical to signal to the model that the null value was structurally missing,\n",
    "    # which is often more informative than the imputed value itself.\n",
    "    for df in [X_train, X_val, X_test]:\n",
    "        # Check if column exists (some may be dropped) and has nulls\n",
    "        if c in df.columns and df[c].isnull().any():\n",
    "            df[f'{c}_is_missing'] = df[c].isnull().astype(int)\n",
    "\n",
    "    # 2. Impute with Median (Original method)\n",
    "    X_train[c] = X_train[c].fillna(median_val)\n",
    "    X_val[c] = X_val[c].fillna(median_val)\n",
    "    X_test[c] = X_test[c].fillna(median_val)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train[num_cols] = scaler.fit_transform(X_train[num_cols])\n",
    "X_val[num_cols] = scaler.transform(X_val[num_cols])\n",
    "X_test[num_cols] = scaler.transform(X_test[num_cols])\n",
    "\n",
    "# --- 5. Cost-Sensitive LightGBM Model (Handling Imbalance) ---\n",
    "\n",
    "# Calculate Class Weights based on inverse frequency (Baseline for Imbalance)\n",
    "class_counts = y_train.value_counts().sort_index()\n",
    "total_samples = len(y_train)\n",
    "# Base weight calculation: total_samples / (num_classes * class_count)\n",
    "base_weights = total_samples / (5 * class_counts)\n",
    "\n",
    "# Manual Adjustment based on COST_MATRIX (High penalty for misclassifying 3 and 4)\n",
    "# This step uses the base frequency weight and scales it further by the business risk.\n",
    "custom_weights = np.array([\n",
    "    base_weights.get(0, 1) * 1.0,   # Class 0 (Lowest Risk/Highest Frequency) - Low scale factor\n",
    "    base_weights.get(1, 1) * 2.0,   # Class 1\n",
    "    base_weights.get(2, 1) * 3.0,   # Class 2\n",
    "    base_weights.get(3, 1) * 10.0,  # Class 3 (High Risk, severe penalty for missing) - High scale factor\n",
    "    base_weights.get(4, 1) * 20.0   # Class 4 (Critical, catastrophic penalty for missing) - Highest scale factor\n",
    "])\n",
    "\n",
    "# Create sample weights array, mapped to each sample in the training data\n",
    "sample_weights = y_train.apply(lambda x: custom_weights[x])\n",
    "print(f\"\\nCustom Class Weights Applied: {custom_weights}\")\n",
    "\n",
    "# LightGBM Parameters - Optimized for Classification\n",
    "params = {\n",
    "    'objective': 'multiclass',\n",
    "    'num_class': 5,\n",
    "    'metric': 'multi_logloss',\n",
    "    'learning_rate': 0.02, # Reduced learning rate for better convergence and precision\n",
    "    'num_leaves': 40,      # Increased complexity\n",
    "    'feature_fraction': 0.8, # Use subset of features on each tree\n",
    "    'bagging_fraction': 0.8, # Use subset of data on each tree\n",
    "    'bagging_freq': 1,\n",
    "    'max_depth': 8,        # Deeper trees\n",
    "    'seed': RANDOM_STATE,\n",
    "    'n_jobs': -1,\n",
    "    'verbosity': -1,\n",
    "    'boosting_type': 'gbdt'\n",
    "}\n",
    "\n",
    "train_data = lgb.Dataset(X_train, label=y_train, weight=sample_weights)\n",
    "val_data = lgb.Dataset(X_val, label=y_val)\n",
    "\n",
    "print(\"\\n--- Starting LightGBM Training (Cost-Sensitive) ---\")\n",
    "model = lgb.train(params,\n",
    "                  train_data,\n",
    "                  valid_sets=[val_data],\n",
    "                  num_boost_round=3000, # Increased rounds\n",
    "                  callbacks=[lgb.early_stopping(stopping_rounds=150, verbose=True),\n",
    "                             lgb.log_evaluation(period=100)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 dir=rtl align=right style=\"line-height:200%;font-family:vazir;color:#0099cc\">\n",
    "<font face=\"vazir\" color=\"#0099cc\">\n",
    " پیش‌بینی برای داده تست و خروجی\n",
    "</font>\n",
    "</h2>\n",
    "\n",
    "<p dir=rtl style=\"direction: rtl;text-align: right;line-height:200%;font-family:vazir;font-size:medium\">\n",
    "<font face=\"vazir\" size=3>\n",
    "    پیش‌بینی مدل خود بر روی داده‌های آزمایش (<code>test</code>) را در قالب یک <code>dataframe</code> در متغیری با نام <code>submission</code>ذخیره کنید.\n",
    "</font>\n",
    "</p>\n",
    "\n",
    "<center>\n",
    "<div dir=rtl style=\"direction: rtl;line-height:200%;font-family:vazir;font-size:medium\">\n",
    "<font face=\"vazir\" size=3>\n",
    "    \n",
    "|class_label|vehicle_id|\n",
    "|---|---|\n",
    "|?|1 |\n",
    "    \n",
    "</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 6. Evaluation and Submission ---\n",
    "\n",
    "y_pred = np.argmax(model.predict(X_val), axis=1)\n",
    "\n",
    "def compute_raw_score(actual, predicted, cost_matrix):\n",
    "    \"\"\"Calculates the final cost-sensitive score based on the business matrix.\"\"\"\n",
    "    score = 0\n",
    "    # Ensure arrays are correctly typed for indexing\n",
    "    actual = np.array(actual, dtype=int)\n",
    "    predicted = np.array(predicted, dtype=int)\n",
    "\n",
    "    # Use advanced indexing for efficient calculation (vectorized operation)\n",
    "    score = np.sum(cost_matrix[actual, predicted])\n",
    "    return score\n",
    "\n",
    "raw = compute_raw_score(y_val, y_pred, COST_MATRIX)\n",
    "perfect_raw = compute_raw_score(y_val, y_val, COST_MATRIX)\n",
    "\n",
    "print(\"\\n--- Validation Results (Cost-Sensitive Score) ---\")\n",
    "# Requires sklearn.metrics.classification_report\n",
    "try:\n",
    "    print(classification_report(y_val, y_pred))\n",
    "except NameError:\n",
    "    # Handle case where classification_report might not be imported\n",
    "    pass\n",
    "print('Validation Confusion Matrix:\\n', confusion_matrix(y_val, y_pred))\n",
    "print('Validation Raw Score:', raw)\n",
    "print('Validation Perfect Raw Score (Benchmark):', perfect_raw)\n",
    "print('Validation Final Score (%):', 100 * max(0, raw) / perfect_raw if perfect_raw > 0 else 0)\n",
    "\n",
    "# Final Prediction and Submission\n",
    "test_pred = np.argmax(model.predict(X_test), axis=1)\n",
    "sub = pd.DataFrame({'vehicle_id': test_final['vehicle_id'], 'class_label': test_pred.astype(int)})\n",
    "sub.sort_values('vehicle_id', inplace=True)\n",
    "sub.to_csv('submission.csv', index=False)\n",
    "print('\\nSaved submission.csv with final predictions.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 dir=rtl align=right style=\"line-height:200%;font-family:vazir;color:#0099cc\">\n",
    "<font face=\"vazir\" color=\"#0099cc\">\n",
    "<b>سلول جواب‌ساز</b>\n",
    "</font>\n",
    "</h2>\n",
    "\n",
    "<p dir=rtl style=\"direction: rtl; text-align: justify; line-height:200%; font-family:vazir; font-size:medium\">\n",
    "<font face=\"vazir\" size=3>\n",
    "    برای ساخته‌شدن فایل <code>result.zip</code> سلول زیر را اجرا کنید. توجه داشته باشید که پیش از اجرای سلول زیر تغییرات اعمال شده در نت‌بوک را ذخیره کرده باشید (<code>ctrl+s</code>) در غیر این صورت، در پایان مسابقه نمره شما به صفر تغییر خواهد کرد.\n",
    "    <br>\n",
    "    همچنین اگر از کولب برای اجرای این فایل نوت‌بوک استفاده می‌کنید، قبل از ارسال فایل <code>result.zip</code>، آخرین نسخه‌ی نوت‌بوک خود را دانلود کرده و داخل فایل ارسالی قرار دهید.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "if not os.path.exists(os.path.join(os.getcwd(), 'notebook.ipynb')):\n",
    "    %notebook -e notebook.ipynb\n",
    "\n",
    "def compress(file_names):\n",
    "    print(\"File Paths:\")\n",
    "    print(file_names)\n",
    "    compression = zipfile.ZIP_DEFLATED\n",
    "    with zipfile.ZipFile(\"result.zip\", mode=\"w\") as zf:\n",
    "        for file_name in file_names:\n",
    "            zf.write('./' + file_name, file_name, compress_type=compression)\n",
    "\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "file_names = ['notebook.ipynb', 'submission.csv']\n",
    "compress(file_names)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
